{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebf4dab4",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18836a8",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "from IPython.utils import io\n",
    "from IPython.display import display, Math, HTML\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import dateutil\n",
    "import time\n",
    "import re\n",
    "import geopy.distance\n",
    "import difflib\n",
    "from scipy.optimize import minimize\n",
    "from functools import partial\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "import json\n",
    "import numbers\n",
    "\n",
    "\n",
    "class TicToc(object):\n",
    "    def __init__(self):\n",
    "        self.last_t = time.time()\n",
    "\n",
    "    def tic(self):\n",
    "        self.last_t = time.time()\n",
    "        return\n",
    "\n",
    "    def print_elapsed(self, dt):\n",
    "        days = int(dt // (3600 * 24))\n",
    "        hours = int((dt - days * 3600 * 24) // 3600)\n",
    "        minutes = int((dt - days * 3600 * 24 - hours * 3600) // 60)\n",
    "        seconds = dt - days * 3600 * 24 - hours * 3600 - minutes * 60\n",
    "\n",
    "        s = \"Elapsed \"\n",
    "        if days > 0:\n",
    "            s += f\"{days} day\" + (\"s, \" if days != 1 else \", \")\n",
    "        if hours > 0 or days > 0:\n",
    "            s += f\"{hours} hour\" + (\"s, \" if hours != 1 else \", \")\n",
    "        if minutes > 0 or hours > 0 or days > 0:\n",
    "            s += f\"{minutes} minute\" + (\"s and \" if minutes != 1 else \" and \")\n",
    "\n",
    "        s += f\"{seconds:1.2f} seconds\"\n",
    "\n",
    "        return s\n",
    "\n",
    "    def toc(self, b_print=True):\n",
    "        elapsed = time.time() - self.last_t\n",
    "        if b_print:\n",
    "            print(self.print_elapsed(elapsed))\n",
    "        else:\n",
    "            return elapsed\n",
    "\n",
    "\n",
    "tictoc = TicToc()\n",
    "\n",
    "\n",
    "class str_print(object):\n",
    "    def __init__(self):\n",
    "        self._s = \"\"\n",
    "\n",
    "    def __call__(self, s=\"\"):\n",
    "        print(s)\n",
    "        self._s += \"\\n\" + s\n",
    "\n",
    "    def get_str(self):\n",
    "        return self._s\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "\n",
    "# Get a personal copy of the data of the author at:\n",
    "# https://drive.google.com/file/d/1GfAB6PpC8voXshiZKADX6KU9FjjY8d7f/view?usp=sharing\n",
    "# Should the link not work, please, drop me an email to guido.gigante@gmail.com\n",
    "data_path = r\"your_path/where_you_unzipped_the_data/Data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc02889",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69675df7",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def signif_digit(x, n, as_string=False):\n",
    "    x = np.array(x)\n",
    "    x0 = x.copy()\n",
    "    x0[x0 == 0.0] = 1.0\n",
    "    k = np.floor(np.log10(np.abs(x0)))\n",
    "    x_round = np.round(x * 10 ** (n - k - 1)) * 10 ** (k - n + 1)\n",
    "    if as_string:\n",
    "        k = np.array(k)\n",
    "        x_round = np.array(x_round)\n",
    "        if k.shape == ():\n",
    "            k = np.array([k])\n",
    "            x_round = np.array([x_round])\n",
    "\n",
    "        x_round = [\n",
    "            eval(f'f\"{{{str(xr)}:1.{max(0, int(n - kx - 1))}f}}\"')\n",
    "            for kx, xr in zip(k, x_round)\n",
    "        ]\n",
    "        if x.shape == ():\n",
    "            return x_round[0]\n",
    "        else:\n",
    "            return np.array(x_round)\n",
    "    else:\n",
    "        return x_round\n",
    "\n",
    "\n",
    "##################################################################################\n",
    "# Adam optimizer\n",
    "def power_lr(step, lr0, scale=4 * 10**5, gamma=0.75):\n",
    "    lr = lr0 / (1 + step / scale) ** gamma\n",
    "\n",
    "    return lr\n",
    "\n",
    "\n",
    "def _adam_1_step(\n",
    "    params,\n",
    "    train_data_it,\n",
    "    f,\n",
    "    v,\n",
    "    sqr,\n",
    "    v_bias_corr,\n",
    "    sqr_bias_corr,\n",
    "    lr_f=lambda step: power_lr(step=step, lr0=10**-3, scale=10**8, gamma=0.75),\n",
    "    step=0,\n",
    "    beta1_f=lambda step: 0.9,\n",
    "    beta2_f=lambda step: 0.999,\n",
    "    eps_stable=1e-8,\n",
    "    mode=\"adam\",\n",
    "    clip_values=[-np.inf, np.inf],\n",
    "):\n",
    "    beta1 = beta1_f(step)\n",
    "    if beta2_f is None:\n",
    "        beta2 = None\n",
    "    else:\n",
    "        beta2 = beta2_f(step)\n",
    "\n",
    "    if mode == \"rmsprop\":\n",
    "        v *= beta1\n",
    "        params -= lr_f(step) * v\n",
    "\n",
    "    # get new batch\n",
    "    batch, f_named_params = next(train_data_it)\n",
    "    # compute gradient\n",
    "    value, g_orig = f(params, batch, **f_named_params)\n",
    "    g = np.clip(g_orig, a_min=clip_values[0], a_max=clip_values[1])\n",
    "\n",
    "    if mode == \"adam\":\n",
    "        v[:] = beta1 * v + (1.0 - beta1) * g\n",
    "\n",
    "        if beta2 is not None and beta2 >= 0.0:\n",
    "            sqr[:] = beta2 * sqr + (1.0 - beta2) * np.square(g)\n",
    "\n",
    "            v_bias_corr[:] = v / (1.0 - beta1 ** (step + 1))\n",
    "            sqr_bias_corr[:] = sqr / (1.0 - beta2 ** (step + 1))\n",
    "\n",
    "            div = lr_f(step) * v_bias_corr / (np.sqrt(sqr_bias_corr) + eps_stable)\n",
    "        else:  # Simple stochastic gradient descent\n",
    "            div = lr_f(step) * v\n",
    "\n",
    "        params -= div\n",
    "    else:  # rmsprop\n",
    "        if beta2 is not None and beta2 >= 0.0:\n",
    "            sqr[:] = beta2 * sqr + (1.0 - beta2) * np.square(g)\n",
    "            sqr_bias_corr[:] = sqr / (1.0 - beta2 ** (step + 1))\n",
    "            # if ((step + 1) % 100) == 0:\n",
    "            # print(np.sqrt(sqr_bias_corr) + eps_stable)\n",
    "            g_norm = g / (np.sqrt(sqr_bias_corr) + eps_stable)\n",
    "            v += g_norm\n",
    "            params -= lr_f(step) * g_norm\n",
    "        else:  # Simple stochastic gradient descent (with momentum)\n",
    "            if np.any(sqr_bias_corr > 0.0):\n",
    "                # g_norm = g / (np.sqrt(sqr_bias_corr) + eps_stable).max()\n",
    "                g_norm = g / (np.sqrt(sqr_bias_corr) + eps_stable)\n",
    "            else:\n",
    "                g_norm = g\n",
    "            v += g_norm\n",
    "            params -= lr_f(step) * v\n",
    "\n",
    "    return value, g_orig\n",
    "\n",
    "\n",
    "def adam(\n",
    "    params0,\n",
    "    train_data_it,\n",
    "    test_data_generator,\n",
    "    f,\n",
    "    n_steps=10**3,\n",
    "    lr_f=lambda step: power_lr(step=step, lr0=10**-3, scale=10**8, gamma=0.75),\n",
    "    beta1=0.9,\n",
    "    beta2=0.999,\n",
    "    eps_stable=1e-8,\n",
    "    params_history_steps=None,\n",
    "    delta_history_steps_min=10,\n",
    "    mode=\"adam\",\n",
    "    stop_f=None,\n",
    "    display_f=None,\n",
    "    internal_data=None,\n",
    "    clip_values=[-np.inf, np.inf],\n",
    "):\n",
    "\n",
    "    msg = \"\"\n",
    "\n",
    "    values = np.zeros((n_steps,))\n",
    "    grads = np.zeros((params0.size, n_steps))\n",
    "\n",
    "    n_history_steps = None\n",
    "    if params_history_steps is None:\n",
    "        n_history_steps = 100\n",
    "    elif type(params_history_steps) != np.ndarray:  # int, np.int32, np.int64...\n",
    "        n_history_steps = params_history_steps\n",
    "\n",
    "    if type(params_history_steps) != np.ndarray:\n",
    "        params_history_steps = (\n",
    "            np.unique(\n",
    "                (\n",
    "                    10 ** np.linspace(np.log10(1), np.log10(n_steps), n_history_steps)\n",
    "                ).astype(int)\n",
    "            )\n",
    "            - 1\n",
    "        )\n",
    "\n",
    "        k0 = 0\n",
    "        for k in range(1, params_history_steps.size):\n",
    "            if (\n",
    "                params_history_steps[k] - params_history_steps[k0]\n",
    "                < delta_history_steps_min\n",
    "            ):\n",
    "                params_history_steps[k] = -1\n",
    "            else:\n",
    "                k0 = k\n",
    "        params_history_steps = params_history_steps[params_history_steps != -1]\n",
    "        del k0, k\n",
    "\n",
    "    k_params_history_steps = 0\n",
    "    params_history = {}\n",
    "    test_error_history = {}\n",
    "\n",
    "    params = params0.copy()\n",
    "    v = np.zeros_like(params)\n",
    "    sqr = np.zeros_like(params)\n",
    "    v_bias_corr = np.zeros_like(params)\n",
    "    sqr_bias_corr = np.zeros_like(params)\n",
    "\n",
    "    if type(beta1) == float:\n",
    "        beta1_f = lambda step: beta1\n",
    "    else:\n",
    "        beta1_f = beta1\n",
    "\n",
    "    if type(beta2) == float:\n",
    "        beta2_f = lambda step: beta2\n",
    "    else:\n",
    "        beta2_f = beta2\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        if internal_data is not None:\n",
    "            internal_data[\"params\"] = params\n",
    "\n",
    "        # print(f\"Adam, step {step + 1}/{n_steps}\")\n",
    "        value, g = _adam_1_step(\n",
    "            params,\n",
    "            train_data_it,\n",
    "            f,\n",
    "            v,\n",
    "            sqr,\n",
    "            v_bias_corr,\n",
    "            sqr_bias_corr,\n",
    "            lr_f=lr_f,\n",
    "            step=step,\n",
    "            beta1_f=beta1_f,\n",
    "            beta2_f=beta2_f,\n",
    "            eps_stable=eps_stable,\n",
    "            mode=mode,\n",
    "            clip_values=clip_values,\n",
    "        )\n",
    "        values[step] = value\n",
    "        grads[:, step] = g\n",
    "\n",
    "        b_add_history = (\n",
    "            k_params_history_steps < params_history_steps.size\n",
    "            and params_history_steps[k_params_history_steps] == step\n",
    "        )\n",
    "        if b_add_history:\n",
    "            params_history[step + 1] = params.copy()\n",
    "\n",
    "            test_err = 0.0\n",
    "            for td, f_named_params in test_data_generator():\n",
    "                value, _ = f(params, td, **f_named_params)\n",
    "                test_err += value\n",
    "\n",
    "            test_error_history[step + 1] = test_err\n",
    "\n",
    "            k_params_history_steps += 1\n",
    "\n",
    "        if display_f is not None:\n",
    "            display_data = {\n",
    "                \"step\": step,\n",
    "                \"params\": params,\n",
    "                \"values\": values[: step + 1],\n",
    "                \"grads\": grads[:, : step + 1],\n",
    "                \"params_history\": params_history,\n",
    "                \"test_error_history\": test_error_history,\n",
    "                \"b_add_history\": b_add_history,\n",
    "            }\n",
    "            display_f(display_data)\n",
    "\n",
    "        if stop_f is not None:\n",
    "            stop_data = {\n",
    "                \"step\": step,\n",
    "                \"params\": params,\n",
    "                \"values\": values[: step + 1],\n",
    "                \"grads\": grads[:, : step + 1],\n",
    "                \"params_history\": params_history,\n",
    "                \"test_error_history\": test_error_history,\n",
    "                \"b_add_history\": b_add_history,\n",
    "            }\n",
    "            b_stop = stop_f(stop_data)\n",
    "\n",
    "            if b_stop:\n",
    "                msg = \"Minimization interrupted by stop function.\"\n",
    "                break\n",
    "\n",
    "    minimize_res = {\n",
    "        \"n_steps\": step + 1,\n",
    "        \"params_history\": params_history,\n",
    "        \"test_error_history\": test_error_history,\n",
    "        \"values\": values[: step + 1],\n",
    "        \"grads\": grads[:, : step + 1],\n",
    "        \"message\": msg,\n",
    "        \"params0\": params0,\n",
    "        \"params\": params,\n",
    "    }\n",
    "\n",
    "    return minimize_res\n",
    "\n",
    "\n",
    "##################################################################################\n",
    "# Functions to save and load json files\n",
    "def _concat_cols(c):\n",
    "    if type(c) == str:\n",
    "        return c\n",
    "    else:\n",
    "        lst = [s for s in c if len(s) > 0]\n",
    "        if len(lst) > 1:\n",
    "            return \"(\" + \", \".join([s for s in c if len(s) > 0]) + \")\"\n",
    "        else:\n",
    "            return lst[0]\n",
    "\n",
    "\n",
    "class NPEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        # if isinstance(obj, np.ndarray):\n",
    "        if callable(getattr(obj, \"tolist\", None)):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, jaxlib.xla_extension.DeviceArray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, pd.core.frame.DataFrame):\n",
    "            return {_concat_cols(c): obj[c].values for c in obj.columns}\n",
    "        elif isinstance(\n",
    "            obj,\n",
    "            (\n",
    "                np.int8,\n",
    "                np.int16,\n",
    "                np.int32,\n",
    "                np.int64,\n",
    "                np.uint8,\n",
    "                np.uint16,\n",
    "                np.uint32,\n",
    "                np.uint64,\n",
    "            ),\n",
    "        ):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, (np.float32, np.float64)):\n",
    "            return float(obj)\n",
    "\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "\n",
    "def check_array(l):\n",
    "    b = isinstance(l, list) and len(l) > 0\n",
    "    if b:\n",
    "        b = np.all([isinstance(x, numbers.Number) for x in l])\n",
    "    return b\n",
    "\n",
    "\n",
    "def check_2d_matrix(l):\n",
    "    b = isinstance(l, list) and len(l) > 0\n",
    "    if b:\n",
    "        b = np.all([check_array(sl) for sl in l])\n",
    "    if b:\n",
    "        lens = np.array([len(sl) for sl in l])\n",
    "        b = np.all(lens == lens[0])\n",
    "    return b\n",
    "\n",
    "\n",
    "def np_obj_hook(o, s=None):\n",
    "    if isinstance(o, dict):\n",
    "        return {item[0]: np_obj_hook(item[1], item[0]) for item in o.items()}\n",
    "    else:\n",
    "        # if s is not None:\n",
    "        # print([s, check_array(o), check_2d_matrix(o)])\n",
    "        # # print([s, o])\n",
    "        if check_array(o) or check_2d_matrix(o):\n",
    "            return np.array(o)\n",
    "        else:\n",
    "            return o\n",
    "\n",
    "\n",
    "def key_to_str(x):\n",
    "    if isinstance(x, dict):\n",
    "        return {str(k): key_to_str(v) for k, v in x.items()}\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "\n",
    "def json_dump(data, file_name):\n",
    "    with open(file_name, \"w\") as outfile:\n",
    "        data = key_to_str(data)\n",
    "        json.dump(data, outfile, cls=NPEncoder, indent=4)\n",
    "\n",
    "\n",
    "def to_int(s):\n",
    "    try:\n",
    "        return int(s)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _data_framing(d_in):\n",
    "    d = d_in.copy()\n",
    "    ln = -1\n",
    "    for k, v in d.items():\n",
    "        b = type(k) == str and type(v) == np.ndarray and len(v.shape) == 1\n",
    "        if b:\n",
    "            if ln == -1:\n",
    "                ln = v.size\n",
    "            else:\n",
    "                b &= v.size == ln\n",
    "        if type(v) == dict:\n",
    "            d[k] = _data_framing(v)\n",
    "\n",
    "    if b:\n",
    "        d = pd.DataFrame.from_dict(d)\n",
    "    return d\n",
    "\n",
    "\n",
    "def _int_keys(d_in):\n",
    "    d = d_in.copy()\n",
    "    b = True\n",
    "    for k, v in d.items():\n",
    "        if b and to_int(k) is None:\n",
    "            b = False\n",
    "\n",
    "        if type(v) == dict:\n",
    "            d[k] = _int_keys(v)\n",
    "\n",
    "    if b:\n",
    "        d = {to_int(k): v for k, v in d.items()}\n",
    "    return d\n",
    "\n",
    "\n",
    "def json_load(file_name):\n",
    "    with open(file_name) as json_file:\n",
    "        data = json.load(json_file, object_hook=np_obj_hook)\n",
    "\n",
    "    # The order is important! If _int_keys does not convert\n",
    "    # str(int) keys to int, _data_framing will convert\n",
    "    # the dictionary to pandas.DataFrame, with columns \"1\", \"23\"... (for example)\n",
    "    data = _int_keys(data)\n",
    "    data = _data_framing(data)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8b40c7",
   "metadata": {},
   "source": [
    "# Creating deaths_per_region.csv from raw data (\"Dataset-decessi-comunali\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a148e22",
   "metadata": {},
   "source": [
    "Load all \"Dataset-decessi-comunali\", massage the data and output that on a single file, <b>deaths_per_region.csv</b> (to be run once - it takes like 25 minutes). <br>\n",
    "<mark>Note that the ZIP file already contains the processed deaths_per_region.csv</mark>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a9e358",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Loading deaths...\n",
    "# Data downloaded at: https://www.istat.it/storage/dati_mortalita/decessi-comunali-giornalieri-regioni(excel)-20-09-2022-5.zip\n",
    "files = [\n",
    "    f\n",
    "    for f in os.listdir(\n",
    "        os.path.join(\n",
    "            data_path,\n",
    "            \"Dataset-decessi-comunali-giornalieri_regioni(excel)_5-21-ottobre-2021\",\n",
    "        )\n",
    "    )\n",
    "    if f.startswith(\"comuni_\")\n",
    "]\n",
    "\n",
    "df = None\n",
    "for k, f in enumerate(files):\n",
    "    _df = pd.read_excel(\n",
    "        os.path.join(\n",
    "            data_path,\n",
    "            \"Dataset-decessi-comunali-giornalieri_regioni(excel)_5-21-ottobre-2021\",\n",
    "            f,\n",
    "        ),\n",
    "        engine=\"openpyxl\",\n",
    "    )\n",
    "\n",
    "    year_cols = [c for c in _df.columns if c.startswith(\"T_\") and c[2:].isnumeric()]\n",
    "    years = [int(c[2:]) for c in year_cols]\n",
    "    _df = pd.melt(\n",
    "        _df[[\"NOME_REGIONE\", \"GE\"] + year_cols],\n",
    "        id_vars=[\"NOME_REGIONE\", \"GE\"],\n",
    "        var_name=\"year\",\n",
    "    )\n",
    "    _df = _df.loc[_df[\"value\"] != \"n.d.\", :]\n",
    "    _df[\"value\"] = _df[\"value\"].astype(int)\n",
    "    del year_cols, years\n",
    "    _df[\"month\"] = _df[\"GE\"] // 100\n",
    "    _df[\"day\"] = _df[\"GE\"] % 100\n",
    "    _df[\"year\"] = _df[\"year\"].transform(lambda s: 2000 + int(s[2:]))\n",
    "    del _df[\"GE\"]\n",
    "    _df = _df[[\"year\", \"month\", \"day\", \"value\", \"NOME_REGIONE\"]].rename(\n",
    "        columns={\"value\": \"deaths\", \"NOME_REGIONE\": \"region\"}\n",
    "    )\n",
    "    _df = (\n",
    "        _df.groupby([\"year\", \"month\", \"day\", \"region\"])\n",
    "        .agg({\"deaths\": \"sum\"})\n",
    "        .reset_index()\n",
    "        .sort_values([\"year\", \"month\", \"day\", \"region\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    _df = _df.groupby([\"region\", \"year\", \"month\"]).agg({\"deaths\": \"sum\"}).reset_index()\n",
    "\n",
    "    if df is None:\n",
    "        df = _df\n",
    "    else:\n",
    "        df = pd.concat((df, _df), axis=0, ignore_index=True)\n",
    "\n",
    "    print(f\"File {k + 1} of {len(files)} loaded...\")\n",
    "\n",
    "del files, f, _df\n",
    "\n",
    "df.to_csv(os.path.join(data_path, \"deaths_per_region.csv\"), index=False)\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9573387c",
   "metadata": {},
   "source": [
    "# Creating \"deaths\" DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee4e3de",
   "metadata": {},
   "source": [
    "Loads <b>deaths</b> DataFrame; for each regione, for each month, for each year in the recording, it gives the recorded number of deaths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0dad9d",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# This directly loads deaths_per_region.csv\n",
    "deaths = pd.read_csv(os.path.join(data_path, \"deaths_per_region.csv\"))\n",
    "deaths = deaths.rename(columns={\"region\": \"regione\"})\n",
    "deaths.loc[deaths[\"regione\"] == \"Emilia-Romagna\", \"regione\"] = \"Emilia Romagna\"\n",
    "deaths.loc[\n",
    "    deaths[\"regione\"] == \"Friuli-Venezia Giulia\", \"regione\"\n",
    "] = \"Friuli Venezia Giulia\"\n",
    "deaths.loc[\n",
    "    deaths[\"regione\"] == \"Trentino-Alto Adige/Südtirol\", \"regione\"\n",
    "] = \"Trentino Alto Adige\"\n",
    "deaths.loc[\n",
    "    deaths[\"regione\"] == \"Valle d'Aosta/Vallée d'Aoste\", \"regione\"\n",
    "] = \"Valle d'Aosta\"\n",
    "\n",
    "deaths.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e511875",
   "metadata": {},
   "source": [
    "# Creating \"capoluoghi\" DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989405ff",
   "metadata": {},
   "source": [
    "Returns <b>capoluoghi</b> (main administrative centres) DataFrame. For each \"capoluogo\", capoluoghi lists the corresponding regione (region), its geographical coordinates, up to three closest weather stations, and the distance to the weather station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030e0299",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Loads: capoluoghi (with closest weather stations), province, pop_regione, and weather_data\n",
    "# Data from: https://github.com/MatteoHenryChinaski/Comuni-Italiani-2018-Sql-Json-excel\n",
    "df = pd.read_excel(\n",
    "    os.path.join(data_path, \"italy_geo.xlsx\"), engine=\"openpyxl\", sheet_name=1\n",
    ")\n",
    "df = df.rename(columns={\"lng\": \"lon\", \"lat\": \"lat\"})\n",
    "df[\"comune\"] = df[\"comune\"].astype(str)\n",
    "df[\"coords\"] = [(x, y) for x, y in zip(df[\"lat\"], df[\"lon\"])]\n",
    "df = df[[\"comune\", \"coords\"]]\n",
    "\n",
    "comuni = df.copy()\n",
    "del df\n",
    "\n",
    "comuni.loc[comuni[\"comune\"] == \"Massa\", \"comune\"] = \"Massa Carrara\"\n",
    "comuni.loc[comuni[\"comune\"] == \"Pesaro\", \"comune\"] = \"Pesaro Urbino\"\n",
    "comuni.loc[comuni[\"comune\"] == \"Iglesias\", \"comune\"] = \"Carbonia Iglesias\"\n",
    "comuni.loc[comuni[\"comune\"] == \"Monza\", \"comune\"] = \"Monza Brianza\"\n",
    "comuni.loc[comuni[\"comune\"] == \"Forlì\", \"comune\"] = \"Forlì Cesena\"\n",
    "\n",
    "# Data from: https://www.dimmicomefare.it/province-italia-sigle/\n",
    "df = pd.read_html(\n",
    "    os.path.join(data_path, \"Province.html\"),\n",
    "    keep_default_na=False,\n",
    "    na_values=[\"##123%%%\"],\n",
    ")\n",
    "df = df[0]\n",
    "df = df.iloc[0:, :]\n",
    "df.columns = df.iloc[0]\n",
    "df = df.drop(df.index[0]).reset_index(drop=True)\n",
    "df = df.rename(\n",
    "    columns={\"Sigla\": \"sigla\", \"Provincia\": \"provincia\", \"Regione\": \"regione\"}\n",
    ")\n",
    "\n",
    "df[\"provincia\"] = df[\"provincia\"].astype(str)\n",
    "df.loc[df[\"regione\"] == \"Valle d’Aosta\", \"regione\"] = \"Valle d'Aosta\"\n",
    "df.loc[df[\"provincia\"] == \"Valle d'Aosta\", \"provincia\"] = \"Aosta\"\n",
    "df[\"sigla\"] = df[\"sigla\"].astype(str).transform(lambda s: s.strip().upper())\n",
    "df = df[[\"sigla\", \"provincia\", \"regione\"]]\n",
    "df[\"regione\"] = df[\"regione\"].transform(lambda s: s.replace(\"-\", \" \"))\n",
    "df[\"provincia\"] = df[\"provincia\"].transform(lambda s: s.replace(\"-\", \" \"))\n",
    "province = df.copy()\n",
    "del df\n",
    "\n",
    "province = province[\n",
    "    ~province[\"provincia\"].isin([\"Medio Campidano\", \"Ogliastra\"])\n",
    "].reset_index(drop=True)\n",
    "province.loc[province[\"provincia\"] == \"Barletta Andria Trani\", \"provincia\"] = \"Barletta\"\n",
    "province.loc[province[\"provincia\"] == \"Olbia Tempio\", \"provincia\"] = \"Olbia\"\n",
    "province.loc[province[\"provincia\"] == \"Forli-Cesena\", \"provincia\"] = \"Forlì\"\n",
    "\n",
    "\n",
    "province[\"provincia_orig\"] = province[\"provincia\"]\n",
    "\n",
    "\n",
    "def f(s):\n",
    "    lst = difflib.get_close_matches(\n",
    "        s,\n",
    "        comuni[\"comune\"].values,\n",
    "        n=1,\n",
    "    )\n",
    "    if len(lst) > 0:\n",
    "        return lst[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "province[\"provincia\"] = province[\"provincia\"].transform(f)\n",
    "province = province.merge(comuni, left_on=\"provincia\", right_on=\"comune\", how=\"left\")\n",
    "del f\n",
    "\n",
    "# Data from: https://www.sport-histoire.fr/it/Geografia/Regioni_Italia.php\n",
    "df = pd.read_html(\n",
    "    os.path.join(data_path, \"Regioni italiane _ regione, capoluogo, superficie.html\"),\n",
    "    keep_default_na=False,\n",
    "    na_values=[\"##123%%%\"],\n",
    ")\n",
    "df = df[0]\n",
    "df = df.rename(\n",
    "    columns={\n",
    "        \"Regione\": \"regione\",\n",
    "        \"Capoluogo\": \"capoluogo\",\n",
    "        df.columns[2]: \"superficie\",\n",
    "    }\n",
    ")\n",
    "df[\"regione\"] = df[\"regione\"].transform(lambda s: s.replace(\"-\", \" \"))\n",
    "df[\"superficie\"] = df[\"superficie\"].astype(str).str.replace(\"\\xa0\", \"\").astype(int)\n",
    "capoluoghi = df.copy()\n",
    "del df\n",
    "\n",
    "capoluoghi[\"capoluogo\"] = capoluoghi[\"capoluogo\"].transform(\n",
    "    lambda s: difflib.get_close_matches(s, comuni[\"comune\"].values, n=1)[0]\n",
    ")\n",
    "capoluoghi = capoluoghi.merge(\n",
    "    comuni, left_on=\"capoluogo\", right_on=\"comune\", how=\"left\"\n",
    ")\n",
    "del capoluoghi[\"comune\"]\n",
    "del comuni\n",
    "\n",
    "\n",
    "df = pd.read_csv(os.path.join(data_path, \"3007097.csv\"))\n",
    "df = df.rename(\n",
    "    columns={\n",
    "        \"LATITUDE\": \"lat\",\n",
    "        \"LONGITUDE\": \"lon\",\n",
    "        \"TAVG\": \"t_avg\",\n",
    "        \"TMAX\": \"t_max\",\n",
    "        \"TMIN\": \"t_min\",\n",
    "        \"STATION\": \"station\",\n",
    "        \"NAME\": \"name\",\n",
    "        \"DATE\": \"date\",\n",
    "        \"ELEVATION\": \"elevation\",\n",
    "    }\n",
    ")\n",
    "df[\"coords\"] = [(x, y) for x, y in zip(df[\"lat\"], df[\"lon\"])]\n",
    "df[\"year\"] = pd.to_datetime(df[\"date\"]).dt.year\n",
    "df[\"month\"] = pd.to_datetime(df[\"date\"]).dt.month\n",
    "df[\"day\"] = pd.to_datetime(df[\"date\"]).dt.day\n",
    "df = df[\n",
    "    [\n",
    "        \"station\",\n",
    "        \"name\",\n",
    "        \"year\",\n",
    "        \"month\",\n",
    "        \"day\",\n",
    "        \"t_avg\",\n",
    "        \"t_min\",\n",
    "        \"t_max\",\n",
    "        \"coords\",\n",
    "        \"elevation\",\n",
    "    ]\n",
    "]\n",
    "weather_data = df.copy()\n",
    "del df\n",
    "\n",
    "df = weather_data.drop_duplicates(\"station\")[[\"station\", \"coords\"]]\n",
    "df = df.merge(capoluoghi[[\"capoluogo\", \"coords\"]], how=\"cross\")\n",
    "\n",
    "\n",
    "def f(p1s, p2s):\n",
    "    ds = np.zeros((p1s.size,))\n",
    "    for k, (p1, p2) in enumerate(zip(p1s, p2s)):\n",
    "        ds[k] = geopy.distance.geodesic(p1, p2).km\n",
    "    return ds\n",
    "\n",
    "\n",
    "df[\"dist\"] = f(df[\"coords_x\"].values, df[\"coords_y\"].values)\n",
    "del f, df[\"coords_x\"], df[\"coords_y\"]\n",
    "\n",
    "df = (\n",
    "    df.sort_values([\"capoluogo\", \"dist\"])\n",
    "    .groupby(\"capoluogo\")\n",
    "    .head(3)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "df[\"dist_min\"] = df.groupby(\"capoluogo\")[\"dist\"].transform(\"min\")\n",
    "\n",
    "df = df[(df[\"dist\"] <= 100.0) | (df[\"dist\"] == df[\"dist_min\"])]\n",
    "del df[\"dist_min\"]\n",
    "\n",
    "df.sort_values(\"dist\", ascending=False)\n",
    "df.sort_values(\"capoluogo\")\n",
    "\n",
    "capoluoghi = capoluoghi.merge(df, on=\"capoluogo\", how=\"left\")\n",
    "\n",
    "###########################################################################\n",
    "# Compute pop_regione\n",
    "df = pd.read_excel(\n",
    "    os.path.join(\n",
    "        data_path,\n",
    "        \"Codici Comuni italiani_1 gennaio 2011.xls\",\n",
    "    ),\n",
    "    sheet_name=\"COMUNI 01_01_2011\",\n",
    ")\n",
    "df = df.rename(\n",
    "    columns={\n",
    "        \"Codice Regione\": \"regione_code\",\n",
    "        \"Codice Provincia\": \"provincia_code\",\n",
    "        \"Solo denominazione in italiano\": \"comune\",\n",
    "        \"Comune capoluogo di provincia\": \"is_capoluogo\",\n",
    "        \"Popolazione residente al 31/12/2009\": \"pop\",\n",
    "    }\n",
    ")\n",
    "df = df[[\"regione_code\", \"provincia_code\", \"comune\", \"is_capoluogo\", \"pop\"]]\n",
    "df = df.merge(\n",
    "    df[df[\"is_capoluogo\"] == 1][[\"provincia_code\", \"comune\"]].rename(\n",
    "        columns={\"comune\": \"capoluogo\"}\n",
    "    ),\n",
    "    on=\"provincia_code\",\n",
    ")\n",
    "\n",
    "df = df.groupby(\"capoluogo\").agg({\"regione_code\": \"first\", \"pop\": \"sum\"}).reset_index()\n",
    "\n",
    "\n",
    "def f(s):\n",
    "    lst = difflib.get_close_matches(\n",
    "        s,\n",
    "        np.unique(df[\"capoluogo\"]),\n",
    "        n=1,\n",
    "    )\n",
    "    if len(lst) > 0:\n",
    "        return lst[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "capoluoghi[\"x\"] = capoluoghi[\"capoluogo\"].transform(f)\n",
    "del f\n",
    "\n",
    "d = {row.x: row.capoluogo for row in capoluoghi.itertuples()}\n",
    "for k, v in d.items():\n",
    "    if k != v:\n",
    "        print([k, v])\n",
    "del capoluoghi[\"x\"]\n",
    "\n",
    "df[\"capoluogo_orig\"] = df[\"capoluogo\"]\n",
    "df[\"capoluogo\"] = df[\"capoluogo\"].map(d)\n",
    "del d\n",
    "\n",
    "\n",
    "def isOneToOne(df, col1, col2):\n",
    "    first = df.drop_duplicates([col1, col2]).groupby(col1)[col2].count().max()\n",
    "    second = df.drop_duplicates([col1, col2]).groupby(col2)[col1].count().max()\n",
    "    #     return (first, second)\n",
    "\n",
    "    return first + second == 2\n",
    "\n",
    "\n",
    "b = isOneToOne(df[~pd.isna(df[\"capoluogo\"])], \"capoluogo\", \"regione_code\")\n",
    "print(f\"Is regione_code and capoluogo one-to-one? {b}\")\n",
    "del b\n",
    "\n",
    "df[\"capoluogo\"] = df.groupby(\"regione_code\")[\"capoluogo\"].transform(\"first\")\n",
    "\n",
    "d = {row.capoluogo: row.regione for row in capoluoghi.itertuples()}\n",
    "df[\"regione\"] = df[\"capoluogo\"].map(d)\n",
    "del d\n",
    "\n",
    "b = isOneToOne(df, \"regione_code\", \"regione\")\n",
    "print(f\"Is regione_code and regione one-to-one? {b}\")\n",
    "del b\n",
    "\n",
    "df = df.groupby(\"regione\").agg({\"pop\": \"sum\"}).reset_index()\n",
    "\n",
    "pop_regione = df.copy()\n",
    "del df\n",
    "\n",
    "# Load Regione-surface\n",
    "# Data from: https://www.sport-histoire.fr/it/Geografia/Regioni_Italia.php\n",
    "df = pd.read_html(\n",
    "    os.path.join(data_path, \"Regioni italiane _ regione, capoluogo, superficie.html\"),\n",
    "    keep_default_na=False,\n",
    "    na_values=[\"##123%%%\"],\n",
    ")\n",
    "df = df[0]\n",
    "df = df.rename(\n",
    "    columns={\n",
    "        \"Regione\": \"regione\",\n",
    "        \"Capoluogo\": \"capoluogo\",\n",
    "        df.columns[2]: \"surface\",\n",
    "    }\n",
    ")\n",
    "df[\"regione\"] = df[\"regione\"].transform(lambda s: s.replace(\"-\", \" \"))\n",
    "df[\"surface\"] = df[\"surface\"].astype(str).str.replace(\"\\xa0\", \"\").astype(int)\n",
    "df = pd.merge(df[[\"regione\", \"surface\"]], pop_regione, on=\"regione\", how=\"right\")\n",
    "df[\"density\"] = df[\"pop\"] / df[\"surface\"]\n",
    "\n",
    "pop_regione = df.copy()\n",
    "del df\n",
    "##############################################################################\n",
    "\n",
    "capoluoghi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85bfbc2",
   "metadata": {},
   "source": [
    "# Creating \"connectivity\" DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4db82fc",
   "metadata": {},
   "source": [
    "This loads <b>connectivity</b> DataFrame (daily people moving from one regione to another)\n",
    "r0 = starting region; r1 = destination region; n = number of people making the trip daily; pop0, pop1 = population of r0 and r1 respectively; pil0 and pil1 are the GDPs of r0 and r1; dist is the distance (in km) between the two regions; adjacent = 1 if the two regions do share a border, and 0 otherwise.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeab595",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Data from: https://www.istat.it/it/archivio/157423\n",
    "df = pd.read_csv(\n",
    "    os.path.join(data_path, \"matrix_pendo2011_10112014.txt\"),\n",
    "    delimiter=r\"\\s+\",\n",
    "    names=[\n",
    "        \"tipo_record\",\n",
    "        \"tipo_residenza\",\n",
    "        \"provincia_residenza\",\n",
    "        \"comune_residenza\",\n",
    "        \"sesso\",\n",
    "        \"motivo\",\n",
    "        \"luogo\",\n",
    "        \"provincia_lavoro\",\n",
    "        \"comune_lavoro\",\n",
    "        \"stato_estero_lavoro\",\n",
    "        \"mezzo\",\n",
    "        \"orario_uscita\",\n",
    "        \"durata\",\n",
    "        \"numero_stimato\",\n",
    "        \"numero\",\n",
    "    ],\n",
    "    low_memory=False,\n",
    ")\n",
    "\n",
    "df = df[[\"provincia_residenza\", \"provincia_lavoro\", \"numero_stimato\", \"numero\"]]\n",
    "df.loc[df[\"numero\"] == \"ND\", \"numero\"] = df.loc[df[\"numero\"] == \"ND\", \"numero_stimato\"]\n",
    "del df[\"numero_stimato\"]\n",
    "df = df.rename(\n",
    "    columns={\n",
    "        \"provincia_residenza\": \"pr0\",\n",
    "        \"provincia_lavoro\": \"pr1\",\n",
    "        \"numero\": \"n\",\n",
    "    }\n",
    ")\n",
    "\n",
    "df = df.loc[df[\"n\"] != \"ND\", :]\n",
    "df[\"n\"] = df[\"n\"].astype(float)\n",
    "\n",
    "df = df.groupby([\"pr0\", \"pr1\"]).agg({\"n\": \"sum\"}).reset_index()\n",
    "df = df[df[\"pr0\"] != df[\"pr1\"]]\n",
    "\n",
    "connectivity = df.copy()\n",
    "del df\n",
    "\n",
    "df = pd.read_excel(\n",
    "    os.path.join(\n",
    "        data_path,\n",
    "        \"Codici Comuni italiani_1 gennaio 2011.xls\",\n",
    "    ),\n",
    "    sheet_name=\"COMUNI 01_01_2011\",\n",
    ")\n",
    "df = df[df[\"Comune capoluogo di provincia\"] == 1].reset_index(drop=True)\n",
    "df = df[[\"Codice Provincia\", \"Solo denominazione in italiano\"]]\n",
    "\n",
    "connectivity = connectivity.merge(\n",
    "    df, left_on=\"pr0\", right_on=\"Codice Provincia\", how=\"inner\"\n",
    ")\n",
    "del connectivity[\"pr0\"], connectivity[\"Codice Provincia\"]\n",
    "connectivity = connectivity.rename(columns={\"Solo denominazione in italiano\": \"pr0\"})\n",
    "\n",
    "connectivity = connectivity.merge(\n",
    "    df, left_on=\"pr1\", right_on=\"Codice Provincia\", how=\"inner\"\n",
    ")\n",
    "del connectivity[\"pr1\"], connectivity[\"Codice Provincia\"]\n",
    "connectivity = connectivity.rename(columns={\"Solo denominazione in italiano\": \"pr1\"})\n",
    "\n",
    "connectivity = connectivity[[\"pr0\", \"pr1\", \"n\"]]\n",
    "\n",
    "del df\n",
    "\n",
    "\n",
    "def f(s):\n",
    "    lst = difflib.get_close_matches(\n",
    "        s,\n",
    "        np.unique(\n",
    "            np.hstack((connectivity[\"pr0\"].unique(), connectivity[\"pr1\"].unique()))\n",
    "        ),\n",
    "        n=1,\n",
    "    )\n",
    "\n",
    "    if len(lst) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return lst[0]\n",
    "\n",
    "\n",
    "province[\"x\"] = province[\"provincia\"].transform(f)\n",
    "d = {row.x: row.provincia for row in province.itertuples()}\n",
    "del province[\"x\"]\n",
    "\n",
    "connectivity[\"pr0\"] = connectivity[\"pr0\"].map(d)\n",
    "connectivity[\"pr1\"] = connectivity[\"pr1\"].map(d)\n",
    "del d\n",
    "\n",
    "connectivity = connectivity.merge(\n",
    "    province[[\"provincia\", \"regione\"]], left_on=\"pr0\", right_on=\"provincia\"\n",
    ")\n",
    "del connectivity[\"provincia\"]\n",
    "connectivity = connectivity.rename(columns={\"regione\": \"r0\"})\n",
    "\n",
    "connectivity = connectivity.merge(\n",
    "    province[[\"provincia\", \"regione\"]], left_on=\"pr1\", right_on=\"provincia\"\n",
    ")\n",
    "del connectivity[\"provincia\"]\n",
    "connectivity = connectivity.rename(columns={\"regione\": \"r1\"})\n",
    "\n",
    "connectivity = connectivity[connectivity[\"r0\"] != connectivity[\"r1\"]]\n",
    "\n",
    "connectivity = connectivity.groupby([\"r0\", \"r1\"]).agg({\"n\": \"sum\"}).reset_index()\n",
    "\n",
    "regioni = np.unique(capoluoghi[\"regione\"])\n",
    "d = {(r.r0, r.r1): r.n for r in connectivity.itertuples()}\n",
    "for r0 in regioni:\n",
    "    for r1 in regioni:\n",
    "        if r1 != r0:\n",
    "            if not (r0, r1) in d:\n",
    "                d[(r0, r1)] = 0.0\n",
    "del r0, r1, regioni\n",
    "d = [(k[0], k[1], v) for k, v in d.items()]\n",
    "\n",
    "df = pd.DataFrame(d, columns=[\"r0\", \"r1\", \"n\"])\n",
    "del d\n",
    "\n",
    "connectivity = df.copy()\n",
    "del df\n",
    "\n",
    "\n",
    "# Add population\n",
    "df = connectivity.copy()\n",
    "df = df.merge(\n",
    "    pop_regione[[\"regione\", \"pop\"]], left_on=\"r0\", right_on=\"regione\", how=\"left\"\n",
    ")\n",
    "df = df.rename(columns={\"pop\": \"pop0\"})\n",
    "del df[\"regione\"]\n",
    "\n",
    "df = df.merge(\n",
    "    pop_regione[[\"regione\", \"pop\"]], left_on=\"r1\", right_on=\"regione\", how=\"left\"\n",
    ")\n",
    "df = df.rename(columns={\"pop\": \"pop1\"})\n",
    "del df[\"regione\"]\n",
    "\n",
    "connectivity = df.copy()\n",
    "del df\n",
    "\n",
    "# Data source: https://it.wikipedia.org/wiki/Regioni_d%27Italia (retrieved on 10/10/2022)\n",
    "df = pd.read_html(\n",
    "    os.path.join(data_path, \"Regioni_d'Italia.htm\"),\n",
    "    keep_default_na=False,\n",
    "    na_values=[\"##123%%%\"],\n",
    ")\n",
    "df = df[4]\n",
    "df = df.rename(\n",
    "    columns={\"Regione o macroregione\": \"regione\", \"PIL totale (mln€)\": \"pil\"}\n",
    ")\n",
    "df = df[df[\"PIL Pro capite (macroregione = 100)\"] != \"-\"]\n",
    "df = df[[\"regione\", \"pil\"]]\n",
    "df = df[df[\"regione\"] != \"Italia\"]\n",
    "df[\"regione\"] = df[\"regione\"].transform(lambda s: s.replace(\" \", \" \"))\n",
    "df[\"regione\"] = df[\"regione\"].transform(lambda s: s.replace(\"-\", \" \"))\n",
    "df[\"pil\"] = df[\"pil\"].transform(lambda s: s.replace(\"\\xa0\", \"\"))\n",
    "df[\"pil\"] = df[\"pil\"].astype(float)\n",
    "df.loc[df[\"regione\"] == \"Trentino\", \"pil\"] += df.loc[\n",
    "    df[\"regione\"] == \"Alto Adige\", \"pil\"\n",
    "].values\n",
    "df.loc[df[\"regione\"] == \"Trentino\", \"regione\"] = \"Trentino Alto Adige\"\n",
    "df = df[df[\"regione\"] != \"Alto Adige\"]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Check regione name alignment\n",
    "regioni = {r: 0 for r in np.unique(capoluoghi[\"regione\"])}\n",
    "for r in df.itertuples():\n",
    "    if r.regione in regioni:\n",
    "        regioni[r.regione] += 1\n",
    "del r\n",
    "regioni = [(k, v) for k, v in regioni.items()]\n",
    "if min(regioni, key=lambda c: c[1])[1] == 0:\n",
    "    print(\"pil_regione.regione misalignment with capoluoghi!\")\n",
    "del regioni\n",
    "\n",
    "pil_regione = df.copy()\n",
    "del df\n",
    "\n",
    "d = {r.regione: r.pil for r in pil_regione.itertuples()}\n",
    "df = connectivity.copy()\n",
    "df[\"pil0\"] = 0.0\n",
    "df[\"pil1\"] = 0.0\n",
    "for r in df.itertuples():\n",
    "    df.loc[r.Index, \"pil0\"] = d[r.r0]\n",
    "    df.loc[r.Index, \"pil1\"] = d[r.r1]\n",
    "\n",
    "df1 = capoluoghi[[\"regione\", \"coords\"]].drop_duplicates(\"regione\")\n",
    "df = (\n",
    "    df.merge(df1, left_on=\"r0\", right_on=\"regione\", how=\"left\")\n",
    "    .drop(\"regione\", axis=\"columns\")\n",
    "    .rename(columns={\"coords\": \"coords0\"})\n",
    ")\n",
    "df = (\n",
    "    df.merge(df1, left_on=\"r1\", right_on=\"regione\", how=\"left\")\n",
    "    .drop(\"regione\", axis=\"columns\")\n",
    "    .rename(columns={\"coords\": \"coords1\"})\n",
    ")\n",
    "del df1\n",
    "\n",
    "\n",
    "def f(p1s, p2s):\n",
    "    ds = np.zeros((p1s.size,))\n",
    "    for k, (p1, p2) in enumerate(zip(p1s, p2s)):\n",
    "        ds[k] = geopy.distance.geodesic(p1, p2).km\n",
    "    return ds\n",
    "\n",
    "\n",
    "df[\"dist\"] = f(df[\"coords0\"].values, df[\"coords1\"].values)\n",
    "del f, df[\"coords0\"], df[\"coords1\"]\n",
    "connectivity = df.copy()\n",
    "del df\n",
    "\n",
    "###################################################################################\n",
    "# Adjacency between regions\n",
    "# Data downloaded from: https://www.istat.it/it/files//2015/04/MatriciContiguita_Province_2011_2018.zip\n",
    "df = pd.read_excel(\n",
    "    os.path.join(\n",
    "        data_path,\n",
    "        \"Contiguità_Province2011.xls\",\n",
    "    ),\n",
    "    sheet_name=\"Contiguita_Province2011\",\n",
    ")\n",
    "df\n",
    "df = df.rename(columns={\"PROVINCIA\": \"pr1\", \"PROVINCIA ADIACENTE\": \"pr2\"})\n",
    "df = df[[\"pr1\", \"pr2\"]]\n",
    "df = df[(~pd.isna(df[\"pr1\"])) & (~pd.isna(df[\"pr2\"]))].reset_index(drop=True)\n",
    "df[\"pr1\"] = df[\"pr1\"].astype(str)\n",
    "df[\"pr2\"] = df[\"pr2\"].astype(str)\n",
    "\n",
    "\n",
    "def f(s):\n",
    "    lst = difflib.get_close_matches(\n",
    "        s,\n",
    "        province[\"provincia\"],\n",
    "        n=1,\n",
    "    )\n",
    "\n",
    "    if len(lst) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return lst[0]\n",
    "\n",
    "\n",
    "d = {s: f(s) for s in np.unique(np.hstack((df[\"pr1\"].unique(), df[\"pr2\"].unique())))}\n",
    "del f\n",
    "\n",
    "df[\"pr1\"] = df[\"pr1\"].map(d)\n",
    "df[\"pr2\"] = df[\"pr2\"].map(d)\n",
    "del d\n",
    "\n",
    "df = df.merge(province[[\"provincia\", \"regione\"]], left_on=\"pr1\", right_on=\"provincia\")\n",
    "del df[\"provincia\"]\n",
    "df = df.rename(columns={\"regione\": \"r1\"})\n",
    "\n",
    "df = df.merge(province[[\"provincia\", \"regione\"]], left_on=\"pr2\", right_on=\"provincia\")\n",
    "del df[\"provincia\"]\n",
    "df = df.rename(columns={\"regione\": \"r2\"})\n",
    "\n",
    "df = df[df[\"r1\"] != df[\"r2\"]]\n",
    "df = df.drop_duplicates([\"r1\", \"r2\"])\n",
    "df = df[[\"r1\", \"r2\"]].sort_values([\"r1\", \"r2\"])\n",
    "\n",
    "regioni_adjacency = df.copy()\n",
    "del df\n",
    "#######################################################################################\n",
    "\n",
    "df = connectivity.copy()\n",
    "regioni_adjacency[\"adjacent\"] = 1\n",
    "df = df.merge(\n",
    "    regioni_adjacency.rename(columns={\"r1\": \"r0\", \"r2\": \"r1\"}),\n",
    "    on=[\"r0\", \"r1\"],\n",
    "    how=\"outer\",\n",
    ")\n",
    "del regioni_adjacency[\"adjacent\"]\n",
    "df[\"adjacent\"] = df[\"adjacent\"].fillna(0).astype(int)\n",
    "\n",
    "connectivity = df.copy()\n",
    "del df\n",
    "\n",
    "\n",
    "connectivity.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8c5366",
   "metadata": {},
   "source": [
    "# Creating \"deaths_t\" DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774ad5c9",
   "metadata": {},
   "source": [
    "It returns <b>deaths_t</b> DataFrame; it gives the same informations (same columns) as <b>deaths</b> DataFrame, adding t_avg, the average temperature in the region in the specific month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dc5135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It combines capoluoghi and weather_data\n",
    "df = weather_data.copy()\n",
    "df = (\n",
    "    df.groupby([\"station\", \"year\", \"month\"])\n",
    "    .agg({\"t_avg\": \"mean\", \"t_min\": \"min\", \"t_max\": \"max\"})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "df = df.merge(capoluoghi[[\"regione\", \"station\", \"dist\"]], on=\"station\", how=\"right\")\n",
    "df = (\n",
    "    df.groupby([\"regione\", \"station\"])\n",
    "    .agg({\"t_avg\": \"mean\", \"dist\": \"first\"})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "\n",
    "ref_t = df.copy()\n",
    "del df\n",
    "\n",
    "\n",
    "df = weather_data.copy()\n",
    "df = (\n",
    "    df.groupby([\"station\", \"year\", \"month\"])\n",
    "    .agg({\"t_avg\": \"mean\", \"t_min\": \"min\", \"t_max\": \"max\"})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "df = df.merge(capoluoghi[[\"regione\", \"station\", \"dist\"]], on=\"station\", how=\"right\")\n",
    "\n",
    "\n",
    "def f(g):\n",
    "    d = {}\n",
    "\n",
    "    ws = 1.0 / g[\"dist\"]\n",
    "    ws /= ws.sum()\n",
    "    d[\"t_avg\"] = (ws * g[\"t_avg\"].values).sum()\n",
    "\n",
    "    return pd.Series(d, dtype=object)\n",
    "\n",
    "\n",
    "df = df.groupby([\"regione\", \"year\", \"month\"]).apply(f).reset_index()\n",
    "del f\n",
    "\n",
    "df = df.merge(deaths, on=[\"regione\", \"year\", \"month\"], how=\"inner\")\n",
    "\n",
    "\n",
    "def f(r):\n",
    "    return datetime(r.year, r.month, 15)\n",
    "\n",
    "\n",
    "df[\"date\"] = df.apply(f, axis=1)\n",
    "\n",
    "deaths_t = df.copy()\n",
    "del df\n",
    "\n",
    "\n",
    "deaths_t.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e0047f",
   "metadata": {},
   "source": [
    "# Figure 1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf8ea55",
   "metadata": {},
   "outputs": [],
   "source": [
    "regioni = [\"Lombardia\", \"Lazio\", \"Sicilia\"]\n",
    "cmap = {\"Lombardia\": \"#6F69AC\", \"Lazio\": \"#FD6F96\", \"Sicilia\": \"#95DAC1\"}\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[14, 12])\n",
    "date_min, date_max = datetime.strptime(\"31/12/2500\", \"%d/%m/%Y\"), datetime.strptime(\n",
    "    \"01/01/1900\", \"%d/%m/%Y\"\n",
    ")\n",
    "for k, regione in enumerate(regioni):\n",
    "    df = deaths_t[deaths_t[\"regione\"] == regione].copy()\n",
    "    df = df[df[\"year\"] <= 2019]\n",
    "    df[\"day\"] = 15\n",
    "    df[\"date\"] = pd.to_datetime(df[[\"year\", \"month\", \"day\"]])\n",
    "\n",
    "    date_min = min(df[\"date\"].min(), date_min)\n",
    "    date_max = max(df[\"date\"].max(), date_max)\n",
    "    ax.plot(\n",
    "        df[\"date\"],\n",
    "        df[\"deaths\"] / df[\"deaths\"].mean(),\n",
    "        \"o-\",\n",
    "        #         c=cmap(k),\n",
    "        c=cmap[regione],\n",
    "        linewidth=6.0,\n",
    "        markersize=12.0,\n",
    "        label=regione,\n",
    "    )\n",
    "\n",
    "ax.legend(fontsize=24)\n",
    "ax.set_ylabel(\"Norm. death rate\", fontsize=24)\n",
    "ax.tick_params(axis=\"both\", which=\"both\", labelsize=20)\n",
    "ax.tick_params(axis=\"x\", rotation=45)\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%b/%y\"))\n",
    "ax.xaxis.set_major_locator(mdates.MonthLocator(bymonth=[1, 7]))\n",
    "date_min -= dateutil.relativedelta.relativedelta(months=1)\n",
    "date_max += dateutil.relativedelta.relativedelta(months=1)\n",
    "ax.set_xlim(date_min, date_max)\n",
    "\n",
    "del regioni, cmap, fig, ax, k, regione, df, date_min, date_max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed752db",
   "metadata": {},
   "source": [
    "# Figure 2a (it needs JAX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136650b7",
   "metadata": {},
   "source": [
    "Add column <i>deaths_pred</i> (number of deaths predicted by the temperature-bi-phasic model) to <b>deaths_t</b> DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d68407",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t_to_use = \"t_avg\"\n",
    "\n",
    "# Bi-phasic model\n",
    "@jax.jit\n",
    "def lam_f(params, ts):\n",
    "    ac, cc, ah, ch, r0 = params\n",
    "    ac = jnp.exp(ac)\n",
    "    ah = jnp.exp(ah)\n",
    "    r0 = jnp.exp(r0)\n",
    "\n",
    "    ec = jnp.exp(jnp.clip(-ac * ts + cc, a_max=100.0))\n",
    "    eh = jnp.exp(jnp.clip(ah * ts + ch, a_max=100.0))\n",
    "\n",
    "    lams = ec + eh  # + r0\n",
    "\n",
    "    return lams\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def poiss_ll(params, ks, ts):\n",
    "    ac, cc, ah, ch, r0 = params\n",
    "    ac = jnp.exp(ac)\n",
    "    ah = jnp.exp(ah)\n",
    "    r0 = jnp.exp(r0)\n",
    "\n",
    "    ec = jnp.exp(jnp.clip(-ac * ts + cc, a_max=100.0))\n",
    "    eh = jnp.exp(jnp.clip(ah * ts + ch, a_max=100.0))\n",
    "\n",
    "    lams = ec + eh  # + r0\n",
    "\n",
    "    ll = (lams - ks * jnp.log(lams)).sum()\n",
    "\n",
    "    return ll\n",
    "\n",
    "\n",
    "regioni = list(deaths_t[\"regione\"].unique())\n",
    "regioni_to_plot = [\"Lombardia\", \"Lazio\", \"Sicilia\"]\n",
    "\n",
    "for i, regione in enumerate(regioni_to_plot):\n",
    "    regioni.insert(i, regioni.pop(regioni.index(regione)))\n",
    "del i, regione\n",
    "\n",
    "cmap = {\"Lombardia\": \"#6F69AC\", \"Lazio\": \"#FD6F96\", \"Sicilia\": \"#95DAC1\"}\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[14, 12])\n",
    "\n",
    "deaths_t[\"deaths_pred\"] = 0.0\n",
    "\n",
    "for k, regione in enumerate(regioni):\n",
    "    df = deaths_t[deaths_t[\"regione\"] == regione].copy()\n",
    "    df = df[df[\"year\"] <= 2019]\n",
    "\n",
    "    g = lambda params: (\n",
    "        poiss_ll(\n",
    "            params,\n",
    "            ks=df[\"deaths\"].values,\n",
    "            ts=df[t_to_use].values,\n",
    "        ),\n",
    "        jax.grad(\n",
    "            lambda params: poiss_ll(\n",
    "                params,\n",
    "                ks=df[\"deaths\"].values,\n",
    "                ts=df[t_to_use].values,\n",
    "            )\n",
    "        )(params),\n",
    "    )\n",
    "\n",
    "    res = minimize(\n",
    "        g,\n",
    "        [\n",
    "            np.log(0.01),\n",
    "            np.log(0.4 * df[\"deaths\"].mean()),\n",
    "            np.log(0.1),\n",
    "            np.log(0.4 * df[\"deaths\"].mean()),\n",
    "            np.log(0.2 * df[\"deaths\"].mean()),\n",
    "        ],\n",
    "        method=\"TNC\",\n",
    "        jac=True,\n",
    "        options={\"xtol\": 10**-6, \"maxfun\": 10**3},\n",
    "        callback=None,\n",
    "    )\n",
    "\n",
    "    if regione in regioni_to_plot:\n",
    "        ac, cc, ah, ch, r0 = res.x\n",
    "        ac = np.exp(ac)\n",
    "        ah = np.exp(ah)\n",
    "        r0 = np.exp(r0)\n",
    "\n",
    "        t0 = (cc - ch + np.log(ac / ah)) / (ac + ah)\n",
    "        del ac, cc, ah, ch, r0\n",
    "\n",
    "        lab = f\"{regione}\"\n",
    "        ax.plot(\n",
    "            (df[t_to_use]),\n",
    "            df[\"deaths\"] / df[\"deaths\"].mean(),\n",
    "            \"o\",\n",
    "            c=cmap[regione],\n",
    "            label=lab,\n",
    "            markersize=10,\n",
    "        )\n",
    "        del t0, lab\n",
    "\n",
    "        x = np.linspace(df[t_to_use].min(), df[t_to_use].max(), 1000)\n",
    "        ax.plot(\n",
    "            x,\n",
    "            (lam_f(res.x, x)) / df[\"deaths\"].mean(),\n",
    "            \"-\",\n",
    "            c=cmap[regione],\n",
    "            linewidth=6.0,\n",
    "        )\n",
    "        del x\n",
    "\n",
    "    deaths_t.loc[deaths_t[\"regione\"] == regione, \"deaths_pred\"] = lam_f(\n",
    "        res.x, deaths_t.loc[deaths_t[\"regione\"] == regione, t_to_use].values\n",
    "    )\n",
    "del g, df, k, regione, t_to_use\n",
    "\n",
    "ax.legend(fontsize=20)\n",
    "ax.set_xlabel(\"T (C)\", fontsize=24)\n",
    "ax.set_ylabel(\"Norm. death rate\", fontsize=24)\n",
    "ax.tick_params(axis=\"both\", which=\"both\", labelsize=20)\n",
    "\n",
    "\n",
    "deaths_t.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ec290e",
   "metadata": {},
   "source": [
    "# Computing corr matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428c3873",
   "metadata": {},
   "source": [
    "Create <b>death_corr</b> DataFrame (with correlation for each pair of regions, both for raw deaths rates and for death reates with the bi-phasic component subtracted - <b>deaths_t</b>[\"deaths_pred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dff37d1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "regioni = sorted(deaths_t[\"regione\"].unique())\n",
    "df = pd.DataFrame(columns=[\"r0\", \"r1\", \"corr0\", \"corr\"])\n",
    "i = 0\n",
    "for k1, regione1 in enumerate(regioni):\n",
    "    df1 = deaths_t[deaths_t[\"regione\"] == regione1].copy()\n",
    "    df1 = df1[df1[\"year\"] <= 2019]\n",
    "    df1 = df1.sort_values([\"regione\", \"year\", \"month\"])\n",
    "    for k2, regione2 in enumerate(regioni):\n",
    "        if k2 <= k1:\n",
    "            pass\n",
    "        else:\n",
    "            df2 = deaths_t[deaths_t[\"regione\"] == regione2].copy()\n",
    "            df2 = df2[df2[\"year\"] <= 2019]\n",
    "            df2 = df2.sort_values([\"regione\", \"year\", \"month\"])\n",
    "\n",
    "            c0 = np.corrcoef(\n",
    "                df1[\"deaths\"],\n",
    "                df2[\"deaths\"],\n",
    "            )[0, 1]\n",
    "\n",
    "            c = np.corrcoef(\n",
    "                df1[\"deaths\"] - df1[\"deaths_pred\"],\n",
    "                df2[\"deaths\"] - df2[\"deaths_pred\"],\n",
    "            )[0, 1]\n",
    "\n",
    "            i += 1\n",
    "            df.loc[i] = [regione1, regione2, c0, c]\n",
    "            i += 1\n",
    "            df.loc[i] = [regione2, regione1, c0, c]\n",
    "\n",
    "death_corr = df.copy()\n",
    "del regioni, df, i, k1, regione1, df1, k2, regione2, df2, c0, c\n",
    "\n",
    "df = death_corr.copy()\n",
    "df = df.merge(pop_regione[[\"regione\", \"pop\"]], left_on=\"r0\", right_on=\"regione\")\n",
    "del df[\"regione\"]\n",
    "df = df.rename(columns={\"pop\": \"pop0\"})\n",
    "df = df.merge(pop_regione[[\"regione\", \"pop\"]], left_on=\"r1\", right_on=\"regione\")\n",
    "del df[\"regione\"]\n",
    "df = df.rename(columns={\"pop\": \"pop1\"})\n",
    "\n",
    "death_corr = df.copy()\n",
    "del df\n",
    "\n",
    "death_corr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cde8a53",
   "metadata": {},
   "source": [
    "# Figures 1b and 2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672a8683",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "regioni = sorted(deaths_t[\"regione\"].unique())\n",
    "regioni = sorted(\n",
    "    [(r.regione, r.pil) for r in pil_regione.itertuples()], key=lambda c: c[1]\n",
    ")[::-1]\n",
    "regioni = [c[0] for c in regioni]\n",
    "\n",
    "df = death_corr.copy()\n",
    "d = {r: k for k, r in enumerate(regioni)}\n",
    "df[\"i\"] = df[\"r0\"].map(d)\n",
    "df[\"j\"] = df[\"r1\"].map(d)\n",
    "del d\n",
    "df = df.sort_values([\"i\", \"j\"])\n",
    "corr0 = np.zeros((len(regioni), len(regioni)))\n",
    "corr = np.zeros((len(regioni), len(regioni)))\n",
    "for r in df.itertuples():\n",
    "    corr0[r.i, r.j] = r.corr0\n",
    "    corr[r.i, r.j] = r.corr\n",
    "del r, df\n",
    "\n",
    "mask = np.zeros_like(corr0, dtype=bool)\n",
    "np.fill_diagonal(mask, True)\n",
    "m, M = min(corr0[corr0 > 0.0].min(), corr[corr > 0.0].min()), max(\n",
    "    corr.max(), corr.max()\n",
    ")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(\n",
    "    corr0, vmin=m, vmax=M, xticklabels=regioni, yticklabels=regioni, mask=mask, ax=ax\n",
    ")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax = sns.heatmap(\n",
    "    corr, vmin=m, vmax=M, xticklabels=regioni, yticklabels=regioni, mask=mask, ax=ax\n",
    ")\n",
    "\n",
    "del mask, fig, ax, corr0, corr, m, M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86227929",
   "metadata": {},
   "source": [
    "# Figure 3a (it needs Jax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa92da5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit on n >= 1\n",
    "df = connectivity.copy()\n",
    "df = df[(df[\"dist\"] < 50000.0) & (df[\"n\"] >= 1)]\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=[\"b_lognorm\"])\n",
    "def f_jit(params, ns, dists, b_lognorm=False):\n",
    "    a, b, c = jnp.exp(params)\n",
    "\n",
    "    lams = a * (jnp.exp(-b * dists) + c)\n",
    "\n",
    "    if b_lognorm:\n",
    "        err = ((jnp.log(lams) - jnp.log(ns)) ** 2).sum()\n",
    "    else:\n",
    "        err = (lams - ns * jnp.log(lams)).sum()\n",
    "\n",
    "    return err\n",
    "\n",
    "\n",
    "params0 = np.array([np.log(4 * 10**4), np.log(1 / 85.0), np.log(10**-5)])\n",
    "\n",
    "g = lambda params: (\n",
    "    f_jit(\n",
    "        params,\n",
    "        ns=df[\"n\"].values,\n",
    "        dists=df[\"dist\"].values,\n",
    "        b_lognorm=True,\n",
    "    ),\n",
    "    jax.grad(\n",
    "        lambda params: f_jit(\n",
    "            params,\n",
    "            ns=df[\"n\"].values,\n",
    "            dists=df[\"dist\"].values,\n",
    "            b_lognorm=True,\n",
    "        )\n",
    "    )(params),\n",
    ")\n",
    "\n",
    "res_lognorm = minimize(\n",
    "    g,\n",
    "    params0,\n",
    "    method=\"TNC\",\n",
    "    jac=True,\n",
    "    options={\"xtol\": 10**-8, \"maxfun\": 10**3},\n",
    "    callback=None,\n",
    ")\n",
    "del g, f_jit, params0\n",
    "\n",
    "# Show everything\n",
    "df = connectivity.copy()\n",
    "df.loc[df[\"n\"] == 0, \"n\"] = 10**-1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[15, 12])\n",
    "\n",
    "ax.plot(df[\"dist\"], df[\"n\"], \"o\", c=\"#557153\", markersize=10)\n",
    "\n",
    "m, M = df[\"dist\"].min(), df[\"dist\"].max()\n",
    "x = np.linspace(m, M, 10**3)\n",
    "\n",
    "\n",
    "y = np.exp(res_lognorm.x[0]) * (\n",
    "    np.exp(-np.exp(res_lognorm.x[1]) * x) + np.exp(res_lognorm.x[2])\n",
    ")\n",
    "lab = f\"$\\propto \\mathrm{{e}}^{{-\\mathrm{{Dist}} / {np.exp(-res_lognorm.x[1]):1.2f}}} + {np.exp(res_lognorm.x[2]):1.2e}$\"\n",
    "ax.plot(x, y, \"-\", c=\"#E6E5A3\", linewidth=6.0, label=lab)\n",
    "del m, M, x, y, lab\n",
    "\n",
    "ax.set_ylim([df[\"n\"].min() * 0.5, None])\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend(fontsize=24)\n",
    "ax.set_xlabel(\"Distance (km)\", fontsize=24)\n",
    "ax.set_ylabel(\n",
    "    r\"$c_{ij}$ (# commuters, region $j$ $\\rightarrow$ region $i{}$)\", fontsize=24\n",
    ")\n",
    "ax.tick_params(axis=\"both\", which=\"both\", labelsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea62629",
   "metadata": {},
   "source": [
    "# Figure 3b (it needs Jax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d0f3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = connectivity.copy()\n",
    "\n",
    "df = df.groupby(\"r1\").agg({\"n\": \"sum\", \"pil1\": \"first\", \"pop1\": \"first\"}).reset_index()\n",
    "df[\"pil1\"] /= 10**3\n",
    "\n",
    "\n",
    "def f(params, ns, pil):\n",
    "    a = params\n",
    "\n",
    "    lams = a * pil\n",
    "    err = ((jnp.log(lams) - jnp.log(ns)) ** 2).sum()\n",
    "    return err\n",
    "\n",
    "\n",
    "f_jit = jax.jit(f)\n",
    "\n",
    "params0 = np.array([200.0])\n",
    "g = lambda params: (\n",
    "    f_jit(\n",
    "        params,\n",
    "        ns=df[\"n\"].values,\n",
    "        pil=df[\"pil1\"].values,\n",
    "    ),\n",
    "    jax.grad(\n",
    "        lambda params: f_jit(\n",
    "            params,\n",
    "            ns=df[\"n\"].values,\n",
    "            pil=df[\"pil1\"].values,\n",
    "        )\n",
    "    )(params),\n",
    ")\n",
    "\n",
    "res = minimize(\n",
    "    g,\n",
    "    params0,\n",
    "    method=\"TNC\",\n",
    "    jac=True,\n",
    "    options={\"xtol\": 10**-6, \"maxfun\": 10**3},\n",
    "    callback=None,\n",
    ")\n",
    "del f, g, f_jit, params0\n",
    "\n",
    "if not res.success:\n",
    "    print(res)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[15, 12])\n",
    "\n",
    "ax.plot(df[\"pil1\"], df[\"n\"] / 10**5, \"o\", c=\"#557153\", markersize=10)\n",
    "\n",
    "m, M = df[\"pil1\"].min(), df[\"pil1\"].max()\n",
    "x = np.linspace(m, M, 10**3)\n",
    "y = res.x[0] * x\n",
    "\n",
    "lab = f\"{signif_digit(res.x[0], 2):1.0f} * $\\mathrm{{GDP}}_{{i}}$\"\n",
    "ax.plot(x, y / 10**5, \"-\", c=\"#E6E5A3\", linewidth=6.0, label=lab)\n",
    "del m, M, x, y, lab\n",
    "\n",
    "ax.set_xlim([0.0, None])\n",
    "ax.set_ylim([0.0, None])\n",
    "ax.legend(fontsize=24)\n",
    "ax.set_xlabel(r\"$\\mathrm{GDP}_{i}$ (M€)\", fontsize=24)\n",
    "ax.set_ylabel(r\"$c_{i:}$ (# commuters to region i) ($x 10^5$)\", fontsize=24)\n",
    "ax.tick_params(axis=\"both\", which=\"both\", labelsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8145e1b3",
   "metadata": {},
   "source": [
    "# Figure 4 (it needs Jax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781db515",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = connectivity.copy()\n",
    "df = df[(df[\"dist\"] < 50000.0) & (df[\"n\"] >= 1)]\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f_jit(params, ns, dists, pop0s, pil1s):\n",
    "    a, b = jnp.exp(params)\n",
    "\n",
    "    lams = a * pil1s * pop0s * jnp.exp(-b * dists)\n",
    "\n",
    "    err = ((jnp.log(lams) - jnp.log(ns)) ** 2).sum()\n",
    "\n",
    "    return err\n",
    "\n",
    "\n",
    "params0 = np.array([np.log(0.01), np.log(1 / 85.0)])\n",
    "g = lambda params: (\n",
    "    f_jit(\n",
    "        params,\n",
    "        ns=df[\"n\"].values,\n",
    "        dists=df[\"dist\"].values,\n",
    "        pop0s=df[\"pop0\"].values,\n",
    "        pil1s=df[\"pil1\"].values,\n",
    "    ),\n",
    "    jax.grad(\n",
    "        lambda params: f_jit(\n",
    "            params,\n",
    "            ns=df[\"n\"].values,\n",
    "            dists=df[\"dist\"].values,\n",
    "            pop0s=df[\"pop0\"].values,\n",
    "            pil1s=df[\"pil1\"].values,\n",
    "        )\n",
    "    )(params),\n",
    ")\n",
    "\n",
    "res = minimize(\n",
    "    g,\n",
    "    params0,\n",
    "    method=\"TNC\",\n",
    "    jac=True,\n",
    "    options={\"xtol\": 10**-8, \"maxfun\": 10**3},\n",
    "    callback=None,\n",
    ")\n",
    "\n",
    "if not res.success:\n",
    "    print(res)\n",
    "\n",
    "a, b = jnp.exp(res.x)\n",
    "print(f\"a = {a:1.3e}, tau = {1 / b:1.3f}\")\n",
    "lams = a * df[\"pil1\"].values * df[\"pop0\"].values * np.exp(-b * df[\"dist\"].values)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[12, 12])\n",
    "ax.axis(\"equal\")\n",
    "\n",
    "ax.plot(df[\"n\"], lams, \"o\", c=\"#557153\", markersize=10)\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "\n",
    "mx, Mx = ax.get_xlim()\n",
    "my, My = ax.get_ylim()\n",
    "\n",
    "m, M = min(df[\"n\"].min(), lams.min()), max(df[\"n\"].max(), lams.max())\n",
    "\n",
    "ax.plot([m, M], [m, M], \"-\", c=\"#E6E5A3\", linewidth=6.0, label=\"y=x\")\n",
    "ax.set_xlim(mx, Mx)\n",
    "ax.set_ylim(my, My)\n",
    "del m, mx, my, M, Mx, My\n",
    "\n",
    "ax.legend(fontsize=24)\n",
    "ax.set_xlabel(r\"$c_{ij}$\", fontsize=24)\n",
    "ax.set_ylabel(\n",
    "    f\"$c_{{ij}}^{{\\mathrm{{fit}}}} \\propto \\mathrm{{pop}}_{{j}} \\, \"\n",
    "    + f\"\\mathrm{{GDP}}_{{i}} \\, \\mathrm{{e}}^{{-\\mathrm{{dist}}_{{ij}} / {1 / b:1.1f}}}$\",\n",
    "    fontsize=24,\n",
    ")\n",
    "ax.tick_params(axis=\"both\", which=\"both\", labelsize=20)\n",
    "del a, b\n",
    "\n",
    "del fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c887dcc5",
   "metadata": {},
   "source": [
    "# SIR (full model) functions (it needs Jax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26a6b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (jitted) Functions\n",
    "@jax.jit\n",
    "def sig(x):\n",
    "    return 1.0 / (1.0 + jnp.exp(-x))\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def inv_sig(y):\n",
    "    return jnp.log(y / (1.0 - y))\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def lambda_update_step(s_i_r, betas, gamma, pops, c, c_sum):\n",
    "    S0, I0, R0 = s_i_r\n",
    "\n",
    "    M = jnp.diag(S0 * (1 - c_sum)) + c * (S0 / pops)\n",
    "    M_sum = M.sum(axis=1)\n",
    "    dI = betas / pops * M_sum * (I0 * (1.0 - c_sum) + (c @ (I0 / pops)))\n",
    "    M = M.T / M_sum\n",
    "    dI = M @ dI\n",
    "\n",
    "    S = S0 - dI\n",
    "    I = I0 + dI - gamma * I0\n",
    "\n",
    "    R = R0 + gamma * I0\n",
    "    return (S, I, R), (S, I, R)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def beta_t_f(abeta, bbeta, ts, densities):\n",
    "    betas = jnp.clip(\n",
    "        jnp.log(densities) * jnp.exp(-abeta * ts + bbeta), a_max=1.0 - 10**-4\n",
    "    )\n",
    "    return betas\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=[\"n_steps_per_month\", \"r_nb\"])\n",
    "def lambda_update_month(\n",
    "    err,\n",
    "    data,\n",
    "    n_steps_per_month,\n",
    "    abeta,\n",
    "    bbeta,\n",
    "    gamma,\n",
    "    mu,\n",
    "    ah,\n",
    "    bh,\n",
    "    rate0s,\n",
    "    pops,\n",
    "    densities,\n",
    "    c,\n",
    "    c_sum,\n",
    "    r_nb=None,\n",
    "):\n",
    "    ns, ts, S0, I0, R0, w = data\n",
    "    betas = beta_t_f(abeta, bbeta, ts, densities)\n",
    "    (S, I, R), s_i_rs = jax.lax.scan(\n",
    "        lambda s_i_r, dummy: lambda_update_step(s_i_r, betas, gamma, pops, c, c_sum),\n",
    "        (S0, I0, R0),\n",
    "        None,\n",
    "        length=n_steps_per_month,\n",
    "    )\n",
    "\n",
    "    deaths_spreading = mu * R\n",
    "    deaths_hot = pops * jnp.exp(ah * ts + bh)\n",
    "    deaths_base = pops * rate0s\n",
    "\n",
    "    mean_deaths = mu * R + deaths_hot + deaths_base\n",
    "    if r_nb is None:  # Poisson\n",
    "        _err = w * (mean_deaths - ns - ns * jnp.log(mean_deaths / ns)).sum()\n",
    "    elif r_nb > 0.0:\n",
    "        ls = mean_deaths\n",
    "        _err = (\n",
    "            w\n",
    "            * (\n",
    "                -ns * (jnp.log(ls / (ls + r_nb)) - jnp.log(ns / (ns + r_nb)))\n",
    "                - r_nb * (jnp.log(r_nb / (ls + r_nb)) - jnp.log(r_nb / (ns + r_nb)))\n",
    "            ).sum()\n",
    "        )\n",
    "    else:  # Gaussian\n",
    "        _err = (-r_nb) * w * ((mean_deaths - ns) ** 2).sum()\n",
    "\n",
    "    err += _err\n",
    "    return err, (\n",
    "        S,\n",
    "        I,\n",
    "        R,\n",
    "        mean_deaths,\n",
    "        _err,\n",
    "        s_i_rs,\n",
    "        deaths_spreading,\n",
    "        deaths_hot,\n",
    "        deaths_base,\n",
    "    )\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def flat_prior(x, params):\n",
    "    x0, x1, alpha = params\n",
    "\n",
    "    def f0(x):\n",
    "        return alpha * (x - x0) ** 2\n",
    "\n",
    "    def f1(x):\n",
    "        return alpha * (x - x1) ** 2\n",
    "\n",
    "    def scan_f(err, x):\n",
    "        _err = jax.lax.cond(\n",
    "            x < x0,\n",
    "            f0,\n",
    "            lambda x: jax.lax.cond(\n",
    "                x > x1,\n",
    "                f1,\n",
    "                lambda x: 0.0,\n",
    "                x,\n",
    "            ),\n",
    "            x,\n",
    "        )\n",
    "\n",
    "        err += _err\n",
    "        return err, _err\n",
    "\n",
    "    err, _ = jax.lax.scan(scan_f, 0.0, x)\n",
    "    err /= x.size\n",
    "\n",
    "    return err\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=[\"n_steps_per_month\", \"r_nb\"])\n",
    "def sir_vec(\n",
    "    params,\n",
    "    n_steps_per_month,\n",
    "    ns,\n",
    "    ts,\n",
    "    S0,\n",
    "    I0,\n",
    "    R0,\n",
    "    c0,\n",
    "    pil1_pop0_m,\n",
    "    dist_m,\n",
    "    pops,\n",
    "    densities,\n",
    "    ws=None,\n",
    "    lambda_corr=None,\n",
    "    corr_array=None,\n",
    "    corr_idxs=None,\n",
    "    r_nb=None,\n",
    "):\n",
    "    alpha, tau, abeta, bbeta, gamma, mu, ah, bh = params[:8]\n",
    "    rate0s = params[8 : 8 + ns.shape[1]]\n",
    "\n",
    "    alpha = jnp.exp(alpha)\n",
    "    tau = jnp.exp(tau)\n",
    "    abeta = jnp.exp(abeta)\n",
    "    gamma = jnp.exp(gamma)\n",
    "    mu = sig(mu)\n",
    "\n",
    "    ah = jnp.exp(ah)\n",
    "    rate0s = jnp.exp(rate0s)\n",
    "\n",
    "    c = c0 + alpha * pil1_pop0_m * jnp.exp(-dist_m / tau)\n",
    "\n",
    "    c_sum = c.sum(axis=0) / pops\n",
    "\n",
    "    if ws is None:\n",
    "        ws = np.ones((ns.shape[0],))\n",
    "\n",
    "    scan_f = lambda s_i_r_err, data: lambda_update_month(\n",
    "        s_i_r_err,\n",
    "        data,\n",
    "        n_steps_per_month,\n",
    "        abeta,\n",
    "        bbeta,\n",
    "        gamma,\n",
    "        mu,\n",
    "        ah,\n",
    "        bh,\n",
    "        rate0s,\n",
    "        pops,\n",
    "        densities,\n",
    "        c,\n",
    "        c_sum,\n",
    "        r_nb,\n",
    "    )\n",
    "    _, s_i_r_ds_errs = jax.lax.scan(\n",
    "        scan_f,\n",
    "        0.0,\n",
    "        (\n",
    "            ns,\n",
    "            ts,\n",
    "            S0,\n",
    "            I0,\n",
    "            R0,\n",
    "            ws,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    errs = s_i_r_ds_errs[4]\n",
    "\n",
    "    if lambda_corr is not None:\n",
    "        deaths = s_i_r_ds_errs[3]\n",
    "\n",
    "        if r_nb is None:\n",
    "            irnb = 0.0\n",
    "        else:\n",
    "            irnb = 1 / r_nb  # It does not work for r_nb < 0!!\n",
    "\n",
    "        def scan_f(errc, cij):\n",
    "            corr0, (i, j) = cij\n",
    "\n",
    "            x, y = deaths[:, i], deaths[:, j]\n",
    "\n",
    "            _corr = ((x - x.mean()) * (y - y.mean())).mean() / (\n",
    "                jnp.sqrt(x.var() + x.mean() + (x**2).mean() * irnb)\n",
    "                * jnp.sqrt(y.var() * (1 + irnb) + y.mean())\n",
    "            )\n",
    "\n",
    "            errc += 0.5 * (_corr - corr0) ** 2\n",
    "\n",
    "            return errc, None\n",
    "\n",
    "        errc, _ = jax.lax.scan(scan_f, 0.0, (corr_array, corr_idxs))\n",
    "        errc /= corr_array.shape[0]\n",
    "\n",
    "        errs += lambda_corr * errc / errs.size\n",
    "\n",
    "    return errs\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=[\"n_steps_per_month\", \"r_nb\"])\n",
    "def sir(\n",
    "    params,\n",
    "    n_steps_per_month,\n",
    "    ns,\n",
    "    ts,\n",
    "    S0,\n",
    "    I0,\n",
    "    R0,\n",
    "    c0,\n",
    "    pil1_pop0_m,\n",
    "    dist_m,\n",
    "    pops,\n",
    "    densities,\n",
    "    ws=None,\n",
    "    lambda_corr=None,\n",
    "    corr_array=None,\n",
    "    corr_idxs=None,\n",
    "    c_sum_prior=None,\n",
    "    r0_prior=None,\n",
    "    tau_prior=None,\n",
    "    kappa_prior=None,\n",
    "    dt_beta_prior=None,\n",
    "    mu_prior=None,\n",
    "    r_nb=None,\n",
    "):\n",
    "    alpha, tau, abeta, bbeta, gamma, mu, ah, bh = params[:8]\n",
    "    rate0s = params[8 : 8 + ns.shape[1]]\n",
    "\n",
    "    alpha = jnp.exp(alpha)\n",
    "    tau = jnp.exp(tau)\n",
    "    abeta = jnp.exp(abeta)\n",
    "    gamma = jnp.exp(gamma)\n",
    "    mu = sig(mu)\n",
    "\n",
    "    ah = jnp.exp(ah)\n",
    "    rate0s = jnp.exp(rate0s)\n",
    "\n",
    "    c = c0 + alpha * pil1_pop0_m * jnp.exp(-dist_m / tau)\n",
    "\n",
    "    c_sum = c.sum(axis=0) / pops\n",
    "\n",
    "    if ws is None:\n",
    "        ws = np.ones((ns.shape[0],))\n",
    "\n",
    "    scan_f = lambda s_i_r_err, data: lambda_update_month(\n",
    "        s_i_r_err,\n",
    "        data,\n",
    "        n_steps_per_month,\n",
    "        abeta,\n",
    "        bbeta,\n",
    "        gamma,\n",
    "        mu,\n",
    "        ah,\n",
    "        bh,\n",
    "        rate0s,\n",
    "        pops,\n",
    "        densities,\n",
    "        c,\n",
    "        c_sum,\n",
    "        r_nb,\n",
    "    )\n",
    "    err, s_i_r_ds_errs = jax.lax.scan(\n",
    "        scan_f,\n",
    "        0.0,\n",
    "        (\n",
    "            ns,\n",
    "            ts,\n",
    "            S0,\n",
    "            I0,\n",
    "            R0,\n",
    "            ws,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    err /= ns.size\n",
    "\n",
    "    if dt_beta_prior is not None:\n",
    "        err += dt_beta_prior[1] * (1 / abeta - dt_beta_prior[0]) ** 2\n",
    "\n",
    "    if c_sum_prior is not None:\n",
    "        err += flat_prior(c_sum, c_sum_prior)\n",
    "\n",
    "    if r0_prior is not None:\n",
    "        r0s = beta_t_f(abeta, bbeta, ts, densities).flatten() / gamma\n",
    "        err += flat_prior(r0s, r0_prior)\n",
    "\n",
    "    if tau_prior is not None:\n",
    "        err += flat_prior(jnp.array([tau]), tau_prior)\n",
    "\n",
    "    if kappa_prior is not None:\n",
    "        err += flat_prior(jnp.array([alpha * tau]), kappa_prior)\n",
    "\n",
    "    if mu_prior is not None:\n",
    "        err += flat_prior(jnp.array([mu]), mu_prior)\n",
    "\n",
    "    if lambda_corr is not None:\n",
    "        deaths = s_i_r_ds_errs[3]\n",
    "\n",
    "        if r_nb is None:\n",
    "            irnb = 0.0\n",
    "        else:\n",
    "            irnb = 1 / r_nb  # It does not work for r_nb < 0!!\n",
    "\n",
    "        def scan_f(errc, cij):\n",
    "            corr0, (i, j) = cij\n",
    "\n",
    "            x, y = deaths[:, i], deaths[:, j]\n",
    "\n",
    "            _corr = ((x - x.mean()) * (y - y.mean())).mean() / (\n",
    "                jnp.sqrt(x.var() + x.mean() + (x**2).mean() * irnb)\n",
    "                * jnp.sqrt(y.var() * (1 + irnb) + y.mean())\n",
    "            )\n",
    "\n",
    "            errc += 0.5 * (_corr - corr0) ** 2\n",
    "\n",
    "            return errc, None\n",
    "\n",
    "        errc, _ = jax.lax.scan(scan_f, 0.0, (corr_array, corr_idxs))\n",
    "        errc /= corr_array.shape[0]\n",
    "\n",
    "        err += lambda_corr * errc\n",
    "\n",
    "    return err\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=[\"n_steps_per_month\", \"r_nb\"])\n",
    "def sir_gen(\n",
    "    params,\n",
    "    n_steps_per_month,\n",
    "    ns,\n",
    "    ts,\n",
    "    S0,\n",
    "    I0,\n",
    "    R0,\n",
    "    c0,\n",
    "    pil1_pop0_m,\n",
    "    dist_m,\n",
    "    pops,\n",
    "    densities,\n",
    "    r_nb=None,\n",
    "):\n",
    "    alpha, tau, abeta, bbeta, gamma, mu, ah, bh = params[:8]\n",
    "    rate0s = params[8 : 8 + ns.shape[1]]\n",
    "\n",
    "    alpha = jnp.exp(alpha)\n",
    "    tau = jnp.exp(tau)\n",
    "    abeta = jnp.exp(abeta)\n",
    "    gamma = jnp.exp(gamma)\n",
    "    mu = sig(mu)\n",
    "\n",
    "    ah = jnp.exp(ah)\n",
    "    rate0s = jnp.exp(rate0s)\n",
    "\n",
    "    c = c0 + alpha * pil1_pop0_m * jnp.exp(-dist_m / tau)\n",
    "\n",
    "    c_sum = c.sum(axis=0) / pops\n",
    "\n",
    "    scan_f = lambda s_i_r_err, data: lambda_update_month(\n",
    "        s_i_r_err,\n",
    "        data,\n",
    "        n_steps_per_month,\n",
    "        abeta,\n",
    "        bbeta,\n",
    "        gamma,\n",
    "        mu,\n",
    "        ah,\n",
    "        bh,\n",
    "        rate0s,\n",
    "        pops,\n",
    "        densities,\n",
    "        c,\n",
    "        c_sum,\n",
    "        r_nb,\n",
    "    )\n",
    "\n",
    "    err, s_i_r_ds = jax.lax.scan(\n",
    "        scan_f,\n",
    "        0.0,\n",
    "        (ns, ts, S0, I0, R0, jnp.ones((ns.shape[0],))),\n",
    "    )\n",
    "\n",
    "    return s_i_r_ds[:3], s_i_r_ds[3], s_i_r_ds[5], s_i_r_ds[6:9]\n",
    "\n",
    "\n",
    "def reals_to_params(reals):\n",
    "    alpha, tau, abeta, bbeta, gamma, mu, ah, bh = reals[:8]\n",
    "    rate0s = reals[8:]\n",
    "\n",
    "    params = [\n",
    "        jnp.log(alpha),\n",
    "        jnp.log(tau),\n",
    "        jnp.log(abeta),\n",
    "        bbeta,\n",
    "        jnp.log(gamma),\n",
    "        inv_sig(mu),\n",
    "        jnp.log(ah),\n",
    "        bh,\n",
    "    ] + [jnp.log(rate0) for rate0 in rate0s]\n",
    "    return params\n",
    "\n",
    "\n",
    "def params_to_reals(params):\n",
    "    alpha, tau, abeta, bbeta, gamma, mu, ah, bh = params[:8]\n",
    "    rate0s = params[8:]\n",
    "\n",
    "    reals = np.array(\n",
    "        [\n",
    "            jnp.exp(alpha),\n",
    "            jnp.exp(tau),\n",
    "            np.exp(abeta),\n",
    "            bbeta,\n",
    "            jnp.exp(gamma),\n",
    "            sig(mu),\n",
    "            jnp.exp(ah),\n",
    "            bh,\n",
    "        ]\n",
    "        + [jnp.exp(rate0) for rate0 in rate0s]\n",
    "    )\n",
    "\n",
    "    return reals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a354eddd",
   "metadata": {},
   "source": [
    "# Load optimised parameters from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13e3ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reloading minimize_data from file\n",
    "minimize_data = json_load(\n",
    "    os.path.join(\n",
    "        data_path,\n",
    "        \"minimize_data.json\",\n",
    "    )\n",
    ")\n",
    "var_names = [\n",
    "    \"regioni\",\n",
    "    \"deaths\",\n",
    "    \"temperatures\",\n",
    "    \"c0\",\n",
    "    \"pops\",\n",
    "    \"densities\",\n",
    "    \"pils\",\n",
    "    \"dist_m\",\n",
    "    \"pil1_pop0_m\",\n",
    "    \"n_steps_a_day\",\n",
    "    \"clip_values\",\n",
    "    \"ps\",\n",
    "    \"c_sum_prior\",\n",
    "    \"tau_prior\",\n",
    "    \"kappa_prior\",\n",
    "    \"r0_prior\",\n",
    "    \"dt_beta_prior\",\n",
    "    \"lambda_corr\",\n",
    "    \"delta_corr_init_step\",\n",
    "    \"n_train_for_corr\",\n",
    "    \"corr_array\",\n",
    "    \"corr_idxs\",\n",
    "    \"test_idxs\",\n",
    "    \"train_idxs\",\n",
    "    \"adam_data\",\n",
    "    \"params0\",\n",
    "    \"minimize_res\",\n",
    "    \"minimize_res_output\",\n",
    "    \"r_nb\",\n",
    "]\n",
    "for var in var_names:\n",
    "    if var not in minimize_data:\n",
    "        print(var)\n",
    "    else:\n",
    "        locals()[var] = minimize_data[var]\n",
    "del var, var_names\n",
    "\n",
    "S0s = np.tile(pops, pops.size).reshape(pops.size, pops.size)\n",
    "I0s = np.zeros_like(S0s)\n",
    "R0s = np.zeros_like(S0s)\n",
    "\n",
    "for i in range(pops.size):\n",
    "    S0s[i, i] -= 1\n",
    "    I0s[i, i] = 1\n",
    "del i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ea7b2e",
   "metadata": {},
   "source": [
    "# Print parameters summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6304239",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "teh = minimize_res[\"test_error_history\"]\n",
    "te_steps = np.array([int(s) for s in teh.keys()])\n",
    "te_values = np.array([e for e in teh.values()])\n",
    "del teh\n",
    "\n",
    "min_step = te_steps[-1]\n",
    "\n",
    "print(f\"min_step = {min_step}\")\n",
    "print()\n",
    "params_opt = minimize_res[\"params_history\"][min_step]\n",
    "del te_steps, te_values, min_step\n",
    "\n",
    "reals = params_to_reals(params_opt)\n",
    "kappa, d0, abeta, bbeta, gamma, mu, ah, bh = reals[:8]\n",
    "ro0s = reals[8:]\n",
    "del reals\n",
    "\n",
    "\n",
    "c = c0 + kappa * pil1_pop0_m * np.exp(-dist_m / d0)\n",
    "c_sum = c.sum(axis=0) / pops\n",
    "\n",
    "print(f\"kappa = {kappa:1.3f}\")\n",
    "print(f\"d0 = {d0:1.1f} Km\")\n",
    "print(f\"Non-commuters account for {(c - c0).sum() / c.sum() * 100:1.3f}% of all flux\")\n",
    "print(\n",
    "    f\"Commuters fraction: max {c_sum.max()} in {regioni[c_sum.argmax()]}, \"\n",
    "    + f\" min {c_sum.min()} in {regioni[c_sum.argmin()]}\"\n",
    ")\n",
    "print()\n",
    "\n",
    "print(f\"mu = {mu * 10**5:1.3f} deaths/10^5\")\n",
    "print()\n",
    "\n",
    "print(\n",
    "    f\"gamma = {gamma:1.4f}; \" + f\"recovery half-life = {np.log(2.0) / gamma:1.1f} days\"\n",
    ")\n",
    "print()\n",
    "\n",
    "if bbeta < 0.0:\n",
    "    print(f\"beta = exp(-(T + {-bbeta / abeta:1.1f} °C) / {1 / abeta:1.1f} °C)\")\n",
    "else:\n",
    "    print(f\"beta = exp(-(T - {bbeta / abeta:1.1f} °C) / {1 / abeta:1.1f} °C)\")\n",
    "\n",
    "betas = beta_t_f(abeta, bbeta, temperatures, densities)\n",
    "print(f\"<beta> = {betas.mean()}\")\n",
    "\n",
    "print(\n",
    "    f\"Doubling time of disease at {temperatures.flatten()[betas.argmax()]:1.2f} °C \"\n",
    "    + f\"= {np.log(2.0) / betas.max():1.1f} days\"\n",
    ")\n",
    "print(\n",
    "    f\"Doubling time of disease at {temperatures.flatten()[betas.argmin()]:1.2f} °C \"\n",
    "    + f\"= {np.log(2.0) / betas.min():1.1f} days\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"R0 at {temperatures.flatten()[betas.argmax()]:1.2f} °C: \"\n",
    "    + f\"{betas.max() / gamma}\"\n",
    ")\n",
    "print(\n",
    "    f\"R0 at {temperatures.flatten()[betas.argmin()]:1.2f} °C: \"\n",
    "    + f\"{betas.min() / gamma}\"\n",
    ")\n",
    "\n",
    "\n",
    "del kappa, d0, abeta, bbeta, gamma, mu, ah, bh, ro0s\n",
    "del c, c_sum, betas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960645c3",
   "metadata": {},
   "source": [
    "# Plot error during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f03045",
   "metadata": {},
   "outputs": [],
   "source": [
    "teh = minimize_res[\"test_error_history\"]\n",
    "te_steps = np.array([int(s) for s in teh.keys()])\n",
    "te_values = np.array([e for e in teh.values()])\n",
    "del teh\n",
    "\n",
    "y = minimize_res[\"values\"]\n",
    "tre_values = np.zeros_like(te_values)\n",
    "\n",
    "s0 = 0\n",
    "for k in range(te_steps.size):\n",
    "    s1 = te_steps[k]\n",
    "    tre_values[k] = y[s0:s1].mean()\n",
    "del s0, s1, k\n",
    "del y\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax1 = ax.twinx()\n",
    "idxs = te_steps >= 0\n",
    "ax.plot(te_steps[idxs], te_values[idxs], \"bo-\")\n",
    "ax1.plot(te_steps[idxs], tre_values[idxs], \"rs-\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlabel(\"Training step\")\n",
    "ax.set_ylabel(\"Test error\")\n",
    "ax1.set_ylabel(\"Train error\")\n",
    "\n",
    "ax.yaxis.label.set_color(\"blue\")\n",
    "ax1.spines[\"left\"].set_color(\"blue\")\n",
    "ax.tick_params(axis=\"y\", colors=\"blue\")\n",
    "\n",
    "ax1.yaxis.label.set_color(\"red\")\n",
    "ax1.spines[\"right\"].set_edgecolor(\"red\")\n",
    "ax1.tick_params(axis=\"y\", colors=\"red\")\n",
    "\n",
    "del te_steps, te_values, tre_values, idxs\n",
    "del fig, ax, ax1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07489f48",
   "metadata": {},
   "source": [
    "# Compute model's predictions (for Figures 5 and 6; they all need Jax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2326237",
   "metadata": {},
   "outputs": [],
   "source": [
    "regioni = list(deaths_t[\"regione\"].unique())\n",
    "\n",
    "teh = minimize_res[\"test_error_history\"]\n",
    "te_steps = np.array([int(s) for s in teh.keys()])\n",
    "te_values = np.array([e for e in teh.values()])\n",
    "del teh\n",
    "\n",
    "min_step = te_steps[-1]\n",
    "params_res = minimize_res[\"params_history\"][min_step]\n",
    "del te_steps, te_values, min_step\n",
    "\n",
    "s_i_rs = None\n",
    "ds = None\n",
    "death_parts = None\n",
    "ev = None\n",
    "for idx in range(len(regioni)):\n",
    "    _s_i_rs, _ds, _ev, _death_parts = sir_gen(\n",
    "        params_res,\n",
    "        30 * n_steps_a_day,\n",
    "        deaths,\n",
    "        temperatures,\n",
    "        S0s[np.repeat(idx, deaths.shape[0]), :],\n",
    "        I0s[np.repeat(idx, deaths.shape[0]), :],\n",
    "        R0s[np.repeat(idx, deaths.shape[0]), :],\n",
    "        c0,\n",
    "        pil1_pop0_m,\n",
    "        dist_m,\n",
    "        pops,\n",
    "        densities,\n",
    "    )\n",
    "\n",
    "    if s_i_rs is None:\n",
    "        s_i_rs = [_s_i_rs[k] * ps[idx] for k in range(3)]\n",
    "        ds = _ds * ps[idx]\n",
    "        death_parts = [_death_parts[k] * ps[idx] for k in range(3)]\n",
    "        ev = [_ev[k] * ps[idx] for k in range(3)]\n",
    "    else:\n",
    "        for k in range(3):\n",
    "            s_i_rs[k] += _s_i_rs[k] * ps[idx]\n",
    "            death_parts[k] += _death_parts[k] * ps[idx]\n",
    "            ev[k] += _ev[k] * ps[idx]\n",
    "        ds += _ds * ps[idx]\n",
    "del _s_i_rs, _ds, k, idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef898c4d",
   "metadata": {},
   "source": [
    "# Figure 5a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715670fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[12, 12])\n",
    "ax.plot(deaths[:], ds[:], \"o\", c=\"#FB2576\", markersize=10)\n",
    "m, M = min(deaths.min(), ds.min()), max(deaths.max(), ds.max())\n",
    "ax.plot([m, M], [m, M], \"-\", c=\"#150050\", linewidth=6.0, label=\"y=x\")\n",
    "\n",
    "ax.set_xlim([m, M])\n",
    "ax.set_ylim([m, M])\n",
    "\n",
    "del m, M\n",
    "\n",
    "ax.legend(fontsize=24)\n",
    "ax.set_xlabel(r\"deaths (per month)\", fontsize=24)\n",
    "ax.set_ylabel(r\"deaths (predicted)\", fontsize=24)\n",
    "ax.tick_params(axis=\"both\", which=\"both\", labelsize=20)\n",
    "\n",
    "print(f\"death corr: {np.corrcoef(deaths.flatten(), ds.flatten())[0, 1]:1.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5900db2",
   "metadata": {},
   "source": [
    "# Figure 5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c197e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(89873903)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[12, 12])\n",
    "\n",
    "cmap = {\"Lombardia\": \"#6F69AC\", \"Lazio\": \"#FD6F96\", \"Sicilia\": \"#95DAC1\"}\n",
    "\n",
    "regioni_to_plot = [\"Sicilia\", \"Lazio\", \"Lombardia\"]\n",
    "\n",
    "df = deaths_t[np.isin(deaths_t[\"regione\"], regioni_to_plot)].copy()\n",
    "df = df[df[\"year\"] <= 2019]\n",
    "\n",
    "date_min, date_max = df[\"date\"].min(), df[\"date\"].max()\n",
    "dates = pd.date_range(date_min, date_max, freq=\"SM\")[::2]\n",
    "del df\n",
    "\n",
    "mms = []\n",
    "preds = []\n",
    "for regione in regioni_to_plot:\n",
    "    n = regioni.index(regione)\n",
    "    if r_nb is None:\n",
    "        y = np.random.poisson(ds[:, n]) / pops[n]\n",
    "    elif r_nb > 0.0:\n",
    "        y = np.random.negative_binomial(r_nb, r_nb / (ds[:, n] + r_nb)) / pops[n]\n",
    "\n",
    "    preds.append(y)\n",
    "    m = min(y.min(), deaths[:, n].min() / pops[n])\n",
    "    M = max(y.max(), deaths[:, n].max() / pops[n])\n",
    "    mms.append((m, M, M - m))\n",
    "del m, M\n",
    "\n",
    "offsets = np.zeros((len(regioni_to_plot),))\n",
    "for i in range(1, len(regioni_to_plot)):\n",
    "    offsets[i] = (\n",
    "        offsets[i - 1] + -mms[i][0] + mms[i - 1][1] - 0.1 * (mms[i - 1][2] + mms[i][2])\n",
    "    )\n",
    "\n",
    "\n",
    "for i, regione in enumerate(regioni_to_plot):\n",
    "    n = regioni.index(regione)\n",
    "    y = preds[i]\n",
    "    ax.plot(\n",
    "        dates,\n",
    "        deaths[:, n] / pops[n] + offsets[i],\n",
    "        \"o--\",\n",
    "        c=cmap[regione],\n",
    "        markersize=8,\n",
    "        linewidth=3.0,\n",
    "    )\n",
    "    ax.plot(\n",
    "        dates,\n",
    "        y + offsets[i],\n",
    "        \"-\",\n",
    "        c=cmap[regione],\n",
    "        linewidth=6.0,\n",
    "        label=f\"{regione}\",\n",
    "    )\n",
    "\n",
    "hs, ls = ax.get_legend_handles_labels()\n",
    "hs = [hs[2], hs[1], hs[0]]\n",
    "ls = [ls[2], ls[1], ls[0]]\n",
    "ax.legend(hs, ls, fontsize=24)\n",
    "del hs, ls\n",
    "\n",
    "ax.set_ylabel(\"deaths (offset)\", fontsize=24)\n",
    "ax.get_yaxis().set_ticks([])\n",
    "ax.tick_params(axis=\"both\", which=\"both\", labelsize=20)\n",
    "\n",
    "ax.tick_params(axis=\"both\", which=\"both\", labelsize=20)\n",
    "ax.tick_params(axis=\"x\", rotation=45)\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%b/%y\"))\n",
    "ax.xaxis.set_major_locator(mdates.MonthLocator(bymonth=[1, 7]))\n",
    "date_min -= dateutil.relativedelta.relativedelta(months=1)\n",
    "date_max += dateutil.relativedelta.relativedelta(months=1)\n",
    "ax.set_xlim(date_min, date_max)\n",
    "\n",
    "del regione, n, regioni_to_plot, y, mms, preds, i, offsets\n",
    "del date_min, date_max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f97cd8c",
   "metadata": {},
   "source": [
    "# Figure 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2762cca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = death_corr.copy()\n",
    "d = {r: k for k, r in enumerate(regioni)}\n",
    "df[\"i\"] = df[\"r0\"].map(d)\n",
    "df[\"j\"] = df[\"r1\"].map(d)\n",
    "del d\n",
    "\n",
    "df = df[df[\"i\"] < df[\"j\"]].sort_values([\"i\", \"j\"])\n",
    "\n",
    "corr0 = np.zeros((len(regioni) * (len(regioni) - 1) // 2))\n",
    "k = 0\n",
    "for r in df.itertuples():\n",
    "    if r.i < r.j:\n",
    "        corr0[k] = r.corr0\n",
    "        k += 1\n",
    "del r, df, k\n",
    "\n",
    "corr = np.zeros_like(corr0)\n",
    "corr_lambda = np.zeros_like(corr0)\n",
    "k = 0\n",
    "if r_nb is None:\n",
    "    irnb = 0.0\n",
    "elif r_nb > 0.0:\n",
    "    irnb = 1 / r_nb\n",
    "else:\n",
    "    irnb = None\n",
    "\n",
    "for i in range(len(regioni)):\n",
    "    for j in range(i + 1, len(regioni)):\n",
    "        x, y = np.array(ds[:, i]), np.array(ds[:, j])\n",
    "        if irnb is not None:\n",
    "            corr[k] = ((x - x.mean()) * (y - y.mean())).mean() / (\n",
    "                np.sqrt(x.var() + x.mean() + (x**2).mean() * irnb)\n",
    "                * np.sqrt(y.var() * (1 + irnb) + y.mean())\n",
    "            )\n",
    "        else:\n",
    "            corr[k] = np.corrcoef(x, y)[0, 1]\n",
    "        corr_lambda[k] = np.corrcoef(x, y)[0, 1]\n",
    "        k += 1\n",
    "del k, i, j, x, y, irnb\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[12, 12])\n",
    "ax.plot(corr0, corr, \"o\", c=\"#0008C1\", markersize=10)\n",
    "m, M = min(corr0.min(), corr.min()), 1.0\n",
    "ax.plot([m, M], [m, M], \"-\", c=\"#E6CBA8\", linewidth=6.0, label=\"y=x\")\n",
    "ax.set_xlim([m, M])\n",
    "ax.set_ylim([m, M])\n",
    "del m, M\n",
    "\n",
    "ax.legend(fontsize=24)\n",
    "ax.set_xlabel(r\"region-region correlation\", fontsize=24)\n",
    "ax.set_ylabel(r\"region-region correlation (predicted)\", fontsize=24)\n",
    "ax.tick_params(axis=\"both\", which=\"both\", labelsize=20)\n",
    "\n",
    "print(f\"corr(corr) = {np.corrcoef(corr0, corr)[0, 1]:1.4f}\")\n",
    "print(f\"corr < corr0 = {(corr <= corr0).mean() * 100:1.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a82cf3",
   "metadata": {},
   "source": [
    "# Optimisation of the full model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535c577c",
   "metadata": {},
   "source": [
    "## Pre-computing the data we need for the optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9fccef",
   "metadata": {},
   "outputs": [],
   "source": [
    "regioni = sorted(deaths_t[\"regione\"].unique())\n",
    "\n",
    "df = deaths_t.loc[deaths_t[\"year\"] <= 2019].copy()\n",
    "df = df.sort_values([\"regione\", \"year\", \"month\"]).reset_index(drop=True)\n",
    "x = df[[\"deaths\", \"t_avg\"]].values\n",
    "deaths = x[:, 0].reshape(len(regioni), x.shape[0] // len(regioni)).T\n",
    "temperatures = x[:, 1].reshape(len(regioni), x.shape[0] // len(regioni)).T\n",
    "del x, df\n",
    "\n",
    "\n",
    "d = {r: k for k, r in enumerate(regioni)}\n",
    "df = connectivity.copy()\n",
    "df[\"j\"] = df[\"r0\"].map(d)\n",
    "df[\"i\"] = df[\"r1\"].map(d)\n",
    "del d\n",
    "\n",
    "c0 = np.zeros((len(regioni), len(regioni)))\n",
    "for r in df.itertuples():\n",
    "    c0[r.i, r.j] = r.n\n",
    "del r\n",
    "\n",
    "pop_dict = {r.regione: (r.pop, r.density) for r in pop_regione.itertuples()}\n",
    "pops = np.zeros((len(regioni),))\n",
    "densities = 10.0 * np.ones((len(regioni),))  # 10 is just not to have 1\n",
    "for i, r in enumerate(regioni):\n",
    "    pops[i] = pop_dict[r][0]\n",
    "#     densities[i] = pop_dict[r][1]\n",
    "del pop_dict, i, r\n",
    "densities = np.exp(np.log(densities) / np.log(densities).mean())\n",
    "\n",
    "pil_dict = {r.regione: r.pil for r in pil_regione.itertuples()}\n",
    "pils = np.zeros((len(regioni),))\n",
    "for i, r in enumerate(regioni):\n",
    "    pils[i] = pil_dict[r]\n",
    "del pil_dict, i, r\n",
    "\n",
    "df = connectivity.copy()\n",
    "d = {r: k for k, r in enumerate(regioni)}\n",
    "df[\"i\"] = df[\"r0\"].map(d)\n",
    "df[\"j\"] = df[\"r1\"].map(d)\n",
    "del d\n",
    "df = df.sort_values([\"i\", \"j\"])\n",
    "dist_m = np.zeros((len(regioni), len(regioni)))\n",
    "for r in df.itertuples():\n",
    "    dist_m[r.i, r.j] = r.dist\n",
    "del r, df\n",
    "\n",
    "pil1_pop0_m = np.ones((len(regioni), len(regioni)))\n",
    "pil1_pop0_m = (pils * pil1_pop0_m.T).T * pops\n",
    "np.fill_diagonal(pil1_pop0_m, 0.0)\n",
    "pil1_pop0_m *= c0.mean() / (pil1_pop0_m * np.exp(-dist_m / 100.0)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d56227",
   "metadata": {},
   "source": [
    "## Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb4bd3b",
   "metadata": {},
   "source": [
    "Expect around 3 hours for a 10^6-step optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f178dca4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "print_ = str_print()\n",
    "\n",
    "n_steps_a_day = 1\n",
    "\n",
    "clip_values = [-100.0, 100.0]\n",
    "\n",
    "r_nb = None\n",
    "\n",
    "ps = pils / pils.sum()\n",
    "\n",
    "S0s = np.tile(pops, pops.size).reshape(pops.size, pops.size)\n",
    "I0s = np.zeros_like(S0s)\n",
    "R0s = np.zeros_like(S0s)\n",
    "\n",
    "for i in range(pops.size):\n",
    "    S0s[i, i] -= 1\n",
    "    I0s[i, i] = 1\n",
    "del i\n",
    "\n",
    "### Setting priors ###\n",
    "r0_prior = None\n",
    "tau_prior = None\n",
    "kappa_prior = None\n",
    "dt_beta_prior = None\n",
    "mu_prior = None\n",
    "lambda_corr = None\n",
    "\n",
    "n = np.median(deaths)\n",
    "delta_n = 10**-2 * n\n",
    "\n",
    "c_sum_prior = [0.0, 0.2, None]\n",
    "delta_c_sum = 10**-5\n",
    "c_sum_prior[2] = delta_n**2 / (2 * n) / delta_c_sum**2\n",
    "del delta_c_sum\n",
    "\n",
    "r0_prior_start = r0_prior\n",
    "\n",
    "tau_prior = [-1.0, 0.0, None]\n",
    "delta_tau = 300.0\n",
    "tau_prior[2] = delta_n**2 / (2 * n) / delta_tau**2\n",
    "del delta_tau\n",
    "\n",
    "delta_corr_init_step = 1 * 10**4\n",
    "n_train_for_corr = 100\n",
    "delta_corr = 0.01\n",
    "lambda_corr = delta_n**2 / (2 * n) / delta_corr**2\n",
    "del delta_corr\n",
    "\n",
    "del n, delta_n\n",
    "\n",
    "###\n",
    "\n",
    "\n",
    "# creating corr vector\n",
    "df = death_corr.copy()\n",
    "d = {r: k for k, r in enumerate(regioni)}\n",
    "df[\"i\"] = df[\"r0\"].map(d)\n",
    "df[\"j\"] = df[\"r1\"].map(d)\n",
    "del d\n",
    "\n",
    "df = df[df[\"i\"] < df[\"j\"]].sort_values([\"i\", \"j\"])\n",
    "corr_array = np.zeros((len(regioni) * (len(regioni) - 1) // 2,))\n",
    "corr_idxs = np.zeros((len(regioni) * (len(regioni) - 1) // 2, 2), dtype=int)\n",
    "k = 0\n",
    "for r in df.itertuples():\n",
    "    if r.i < r.j:\n",
    "        corr_array[k] = r.corr0\n",
    "        corr_idxs[k, :] = r.i, r.j\n",
    "        k += 1\n",
    "del r, df, k\n",
    "\n",
    "# As test indexes I want at least one examplar for each month (12 examplars).\n",
    "# If you have more than 12 years, you just keep one random month for each year.\n",
    "# If years are (as in our case) less than 12, some of the years will contribute\n",
    "# more than one month, paying attention - in this case - that the contributed\n",
    "# months were 6 months apart (like June and December, or April and October).\n",
    "test_idxs = []\n",
    "ys = np.arange(deaths.shape[0] // 12)\n",
    "np.random.shuffle(ys)\n",
    "ky = 0\n",
    "for m in range(max(12, ys.size)):\n",
    "    if ky < ys.size:\n",
    "        _ky = ky\n",
    "    else:\n",
    "        _ky = (ky + 6) % 12\n",
    "\n",
    "    ky += 1\n",
    "\n",
    "    _m = m % 12\n",
    "\n",
    "    test_idxs.append(ys[_ky] * 12 + _m)\n",
    "del ys, ky, m, _ky, _m\n",
    "\n",
    "test_idxs = np.array(test_idxs)\n",
    "\n",
    "months = [\n",
    "    \"Jan\",\n",
    "    \"Feb\",\n",
    "    \"Mar\",\n",
    "    \"Apr\",\n",
    "    \"May\",\n",
    "    \"Jun\",\n",
    "    \"Jul\",\n",
    "    \"Aug\",\n",
    "    \"Sep\",\n",
    "    \"Oct\",\n",
    "    \"Nov\",\n",
    "    \"Dec\",\n",
    "]\n",
    "print_(\", \".join([f\"{months[k % 12]} {k // 12 + 2011}\" for k in test_idxs]))\n",
    "print_()\n",
    "del months\n",
    "\n",
    "\n",
    "def test_data_generator():\n",
    "    regione_idxs = np.repeat(np.arange(len(regioni)), test_idxs.size)\n",
    "    idxs = np.tile(test_idxs, len(regioni))\n",
    "\n",
    "    yield (\n",
    "        deaths[idxs],\n",
    "        temperatures[idxs],\n",
    "        S0s[regione_idxs, :],\n",
    "        I0s[regione_idxs, :],\n",
    "        R0s[regione_idxs, :],\n",
    "    ), {\n",
    "        \"ws\": ps[regione_idxs],\n",
    "        \"lambda_corr\": lambda_corr,\n",
    "        \"c_sum_prior\": None,\n",
    "        \"r0_prior\": None,\n",
    "        \"tau_prior\": None,\n",
    "        \"kappa_prior\": None,\n",
    "        \"dt_beta_prior\": None,\n",
    "        \"mu_prior\": None,\n",
    "    }\n",
    "\n",
    "\n",
    "train_idxs = np.setdiff1d(np.arange(deaths.shape[0]), test_idxs)\n",
    "\n",
    "adam_data = {\"params\": None}\n",
    "\n",
    "\n",
    "def train_data_generator(adam_data=None):\n",
    "    n_calls = 0\n",
    "\n",
    "    while True:\n",
    "        idxs = np.random.choice(\n",
    "            train_idxs,\n",
    "            replace=True,\n",
    "            size=10\n",
    "            if lambda_corr is None or n_calls < delta_corr_init_step\n",
    "            else n_train_for_corr,\n",
    "        )\n",
    "        regione_idxs = np.random.choice(np.arange(ps.size), idxs.size, p=ps)\n",
    "\n",
    "        if n_calls < 5 * 10**4:\n",
    "            _r0_prior = r0_prior_start\n",
    "        else:\n",
    "            _r0_prior = r0_prior\n",
    "\n",
    "        n_calls += 1\n",
    "\n",
    "        yield (\n",
    "            deaths[idxs],\n",
    "            temperatures[idxs],\n",
    "            S0s[regione_idxs, :],\n",
    "            I0s[regione_idxs, :],\n",
    "            R0s[regione_idxs, :],\n",
    "        ), {\n",
    "            \"ws\": np.ones((idxs.size,)),\n",
    "            \"lambda_corr\": lambda_corr if n_calls >= delta_corr_init_step else None,\n",
    "            \"c_sum_prior\": c_sum_prior,\n",
    "            \"r0_prior\": _r0_prior,\n",
    "            \"tau_prior\": tau_prior,\n",
    "            \"kappa_prior\": kappa_prior,\n",
    "            \"dt_beta_prior\": dt_beta_prior,\n",
    "            \"mu_prior\": mu_prior,\n",
    "        }\n",
    "\n",
    "\n",
    "def beta1_f(step):\n",
    "    return 0.9\n",
    "\n",
    "\n",
    "def lr_f(step):\n",
    "    return power_lr(step=step, lr0=1 * 10**-3, scale=10**4, gamma=0.75)\n",
    "\n",
    "\n",
    "def display_f(d):\n",
    "    if (d[\"step\"] + 1) % 10**4 == 0:\n",
    "        reals = params_to_reals(d[\"params\"])\n",
    "        kappa, d0, abeta, bbeta, gamma, mu, ah, bh = reals[:8]\n",
    "        rate0s = reals[8:]\n",
    "\n",
    "        print_(f\"step = {d['step'] + 1}, kappa = {kappa:1.3e}, d0 = {d0:1.2f}\")\n",
    "\n",
    "        betas = beta_t_f(abeta, bbeta, temperatures, densities)\n",
    "\n",
    "        print_(\n",
    "            f\"    R0 at {temperatures.flatten()[betas.argmax()]:1.2f} °C: \"\n",
    "            + f\"{betas.max() / gamma}\"\n",
    "        )\n",
    "        print_(\n",
    "            f\"    R0 at {temperatures.flatten()[betas.argmin()]:1.2f} °C: \"\n",
    "            + f\"{betas.min() / gamma}\"\n",
    "        )\n",
    "\n",
    "        c = c0 + kappa * pil1_pop0_m * np.exp(-dist_m / d0)\n",
    "        c_sum = c.sum(axis=0) / pops\n",
    "\n",
    "        print_(\n",
    "            f\"    Non-commuters account for {(c - c0).sum() / c.sum() * 100:1.3f}% of all flux\"\n",
    "        )\n",
    "        print_(\n",
    "            f\"    Commuters fraction: max {c_sum.max()} in {regioni[c_sum.argmax()]}, \"\n",
    "            + f\" min {c_sum.min()} in {regioni[c_sum.argmin()]}\"\n",
    "        )\n",
    "        print_(f\"    mu = {mu * 10**5:1.3f} deaths/10^5\")\n",
    "\n",
    "        print_()\n",
    "    return\n",
    "\n",
    "\n",
    "ro0s = deaths.mean(axis=0) / pops  # (deaths / pops).mean()\n",
    "params0 = np.array(\n",
    "    [\n",
    "        0.1,  # 1.0 + np.random.rand() * 9,\n",
    "        100.0,\n",
    "        1 / 100.0,\n",
    "        np.log(\n",
    "            (1 - 0.5 ** (1 / (n_steps_a_day * 2.0)))\n",
    "            * np.exp(temperatures.min() / 100.0)\n",
    "        ),\n",
    "        1 - 0.5 ** (1 / (n_steps_a_day * 4.0)),\n",
    "        1.0 / 10**5,\n",
    "        1 / 100.0,\n",
    "        np.log(0.01 * ro0s.mean() * np.exp(-temperatures.max() / 100.0)),\n",
    "    ]\n",
    "    + [ro0 for ro0 in ro0s]\n",
    ")\n",
    "del ro0s\n",
    "params0[:] = reals_to_params(params0)\n",
    "\n",
    "\n",
    "tictoc.tic()\n",
    "minimize_res = adam(\n",
    "    params0,\n",
    "    train_data_generator(adam_data),\n",
    "    test_data_generator,\n",
    "    lambda params, data_tuple, ws, lambda_corr, c_sum_prior, r0_prior, tau_prior, kappa_prior, dt_beta_prior, mu_prior: (\n",
    "        sir(\n",
    "            params,\n",
    "            30 * n_steps_a_day,\n",
    "            data_tuple[0],\n",
    "            data_tuple[1],\n",
    "            data_tuple[2],\n",
    "            data_tuple[3],\n",
    "            data_tuple[4],\n",
    "            c0 / n_steps_a_day,\n",
    "            pil1_pop0_m / n_steps_a_day,\n",
    "            dist_m,\n",
    "            pops,\n",
    "            densities,\n",
    "            ws,\n",
    "            lambda_corr,\n",
    "            corr_array,\n",
    "            corr_idxs,\n",
    "            c_sum_prior,\n",
    "            r0_prior,\n",
    "            tau_prior,\n",
    "            kappa_prior,\n",
    "            dt_beta_prior,\n",
    "            mu_prior,\n",
    "            r_nb,\n",
    "        ),\n",
    "        jax.grad(\n",
    "            lambda params: sir(\n",
    "                params,\n",
    "                30 * n_steps_a_day,\n",
    "                data_tuple[0],\n",
    "                data_tuple[1],\n",
    "                data_tuple[2],\n",
    "                data_tuple[3],\n",
    "                data_tuple[4],\n",
    "                c0 / n_steps_a_day,\n",
    "                pil1_pop0_m / n_steps_a_day,\n",
    "                dist_m,\n",
    "                pops,\n",
    "                densities,\n",
    "                ws,\n",
    "                lambda_corr,\n",
    "                corr_array,\n",
    "                corr_idxs,\n",
    "                c_sum_prior,\n",
    "                r0_prior,\n",
    "                tau_prior,\n",
    "                kappa_prior,\n",
    "                dt_beta_prior,\n",
    "                mu_prior,\n",
    "                r_nb,\n",
    "            )\n",
    "        )(params),\n",
    "    ),\n",
    "    n_steps=10**6,\n",
    "    lr_f=lr_f,\n",
    "    beta1=beta1_f,\n",
    "    beta2=0.99,\n",
    "    eps_stable=1e-8,\n",
    "    params_history_steps=None,\n",
    "    mode=\"adam\",\n",
    "    stop_f=None,\n",
    "    display_f=display_f,\n",
    "    internal_data=adam_data,\n",
    "    clip_values=clip_values,\n",
    ")\n",
    "tictoc.toc()\n",
    "\n",
    "minimize_res_output = print_.get_str()\n",
    "\n",
    "del print_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6269d725",
   "metadata": {},
   "source": [
    "## Save the optimisation results on file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1319c0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save minimize_data to (json) file\n",
    "d = [\n",
    "    (\"regioni\", regioni),\n",
    "    (\"deaths\", deaths),\n",
    "    (\"temperatures\", temperatures),\n",
    "    (\"c0\", c0),\n",
    "    (\"pops\", pops),\n",
    "    (\"densities\", densities),\n",
    "    (\"pils\", pils),\n",
    "    (\"dist_m\", dist_m),\n",
    "    (\"pil1_pop0_m\", pil1_pop0_m),\n",
    "    (\"n_steps_a_day\", n_steps_a_day),\n",
    "    (\"clip_values\", clip_values),\n",
    "    (\"ps\", ps),\n",
    "    (\"c_sum_prior\", c_sum_prior),\n",
    "    (\"r0_prior\", r0_prior),\n",
    "    (\"tau_prior\", tau_prior),\n",
    "    (\"kappa_prior\", kappa_prior),\n",
    "    (\"dt_beta_prior\", dt_beta_prior),\n",
    "    (\"lambda_corr\", lambda_corr),\n",
    "    (\"delta_corr_init_step\", delta_corr_init_step),\n",
    "    (\"n_train_for_corr\", n_train_for_corr),\n",
    "    (\"corr_array\", corr_array),\n",
    "    (\"corr_idxs\", corr_idxs),\n",
    "    (\"test_idxs\", test_idxs),\n",
    "    (\"train_idxs\", train_idxs),\n",
    "    (\"adam_data\", adam_data),\n",
    "    (\"params0\", params0),\n",
    "    (\"minimize_res\", minimize_res),\n",
    "    (\"minimize_res_output\", minimize_res_output),\n",
    "    (\"r_nb\", r_nb),\n",
    "]\n",
    "d = {c[0]: c[1] for c in d}\n",
    "\n",
    "\n",
    "def safe_getsource(f):\n",
    "    try:\n",
    "        return inspect.getsource(f)\n",
    "    except:\n",
    "        print(f\"Unable to retrieve source for {f.__name__}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "d_f = {\n",
    "    f.__name__: safe_getsource(f)\n",
    "    for f in globals().values()\n",
    "    if callable(f)\n",
    "    and getattr(f, \"__name__\", None) is not None\n",
    "    and f.__name__\n",
    "    in [\n",
    "        \"sig\",\n",
    "        \"inv_sig\",\n",
    "        \"gammaln_nr\",\n",
    "        \"sat_id\",\n",
    "        \"lambda_update_step\",\n",
    "        \"beta_t_f\",\n",
    "        \"lambda_update_month\",\n",
    "        \"flat_prior\",\n",
    "        \"sir_vec\",\n",
    "        \"sir\",\n",
    "        \"sir_gen\",\n",
    "        \"reals_to_params\",\n",
    "        \"params_to_reals\",\n",
    "        \"test_data_generator\",\n",
    "        \"train_data_generator\",\n",
    "        \"beta1_f\",\n",
    "        \"lr_f\",\n",
    "        \"display_f\",\n",
    "    ]\n",
    "}\n",
    "del safe_getsource\n",
    "\n",
    "d = {**d, **d_f}\n",
    "del d_f\n",
    "\n",
    "json_dump(\n",
    "    d,\n",
    "    os.path.join(\n",
    "        data_path,\n",
    "        \"minimize_data_new.json\",\n",
    "    ),\n",
    ")\n",
    "del d"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:jax]",
   "language": "python",
   "name": "conda-env-jax-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "109px",
    "width": "208px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "279.672px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
